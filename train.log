The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-10-02 14:56:32 INFO     Using DreamBooth method.                                                                   train_network.py:517
                    INFO     prepare images.                                                                              train_util.py:2072
                    INFO     get image size from name of cache files                                                      train_util.py:1965
  0%|          | 0/172 [00:00<?, ?it/s]100%|██████████| 172/172 [00:00<00:00, 1143296.81it/s]
                    INFO     set image size from cache files: 0/172                                                       train_util.py:1995
                    INFO     found directory /workspace/LoRA/Navya/2_Navya contains 172 image files                       train_util.py:2019
read caption:   0%|          | 0/172 [00:00<?, ?it/s]read caption: 100%|██████████| 172/172 [00:00<00:00, 66076.23it/s]
                    INFO     344 train images with repeats.                                                               train_util.py:2116
                    INFO     0 reg images with repeats.                                                                   train_util.py:2120
                    WARNING  no regularization images / 正則化画像が見つかりませんでした                                  train_util.py:2125
                    INFO     [Dataset 0]                                                                                  config_util.py:580
                               batch_size: 5                                                                                                
                               resolution: (1024, 1024)                                                                                     
                               resize_interpolation: None                                                                                   
                               enable_bucket: True                                                                                          
                               min_bucket_reso: 256                                                                                         
                               max_bucket_reso: 1024                                                                                        
                               bucket_reso_steps: 64                                                                                        
                               bucket_no_upscale: False                                                                                     
                                                                                                                                            
                               [Subset 0 of Dataset 0]                                                                                      
                                 image_dir: "/workspace/LoRA/Navya/2_Navya"                                                                 
                                 image_count: 172                                                                                           
                                 num_repeats: 2                                                                                             
                                 shuffle_caption: False                                                                                     
                                 keep_tokens: 0                                                                                             
                                 caption_dropout_rate: 0.0                                                                                  
                                 caption_dropout_every_n_epochs: 0                                                                          
                                 caption_tag_dropout_rate: 0.0                                                                              
                                 caption_prefix: None                                                                                       
                                 caption_suffix: None                                                                                       
                                 color_aug: False                                                                                           
                                 flip_aug: False                                                                                            
                                 face_crop_aug_range: None                                                                                  
                                 random_crop: False                                                                                         
                                 token_warmup_min: 1,                                                                                       
                                 token_warmup_step: 0,                                                                                      
                                 alpha_mask: False                                                                                          
                                 resize_interpolation: None                                                                                 
                                 custom_attributes: {}                                                                                      
                                 is_reg: False                                                                                              
                                 class_tokens: Navya                                                                                        
                                 caption_extension: .txt                                                                                    
                                                                                                                                            
                                                                                                                                            
                    INFO     [Prepare dataset 0]                                                                          config_util.py:592
                    INFO     loading image sizes.                                                                          train_util.py:987
  0%|          | 0/172 [00:00<?, ?it/s]100%|██████████| 172/172 [00:00<00:00, 110885.38it/s]
                    INFO     make buckets                                                                                 train_util.py:1010
                    INFO     number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）              train_util.py:1056
                    INFO     bucket 0: resolution (1024, 1024), count: 344                                                train_util.py:1061
                    INFO     mean ar error (without repeats): 0.0                                                         train_util.py:1069
                    INFO     preparing accelerator                                                                      train_network.py:580
                    INFO     loading model for process 0/1                                                             sdxl_train_util.py:32
                    INFO     load Diffusers pretrained models: /workspace/RealVisXL_V5.0, variant=None                 sdxl_train_util.py:87
accelerator device: cuda
Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:00, 18.89it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 33.14it/s]
Loading pipeline components...:  67%|██████▋   | 4/6 [00:00<00:00, 16.26it/s]Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 21.76it/s]
                    INFO     U-Net converted to original U-Net                                                        sdxl_train_util.py:123
2025-10-02 14:56:33 INFO     Enable memory efficient attention for U-Net                                                  train_util.py:3346
                    INFO     create LoRA network. base dim (rank): 96, alpha: 96.0                                               lora.py:935
                    INFO     neuron dropout: p=None, rank dropout: p=None, module dropout: p=None                                lora.py:936
                    INFO     create LoRA for Text Encoder 1:                                                                    lora.py:1027
                    INFO     create LoRA for Text Encoder 2:                                                                    lora.py:1027
                    INFO     create LoRA for Text Encoder: 264 modules.                                                         lora.py:1035
2025-10-02 14:56:35 INFO     create LoRA for U-Net: 722 modules.                                                                lora.py:1043
                    INFO     enable LoRA for text encoder: 264 modules                                                          lora.py:1084
                    INFO     enable LoRA for U-Net: 722 modules                                                                 lora.py:1089
                    INFO     use Prodigy optimizer | {}                                                                   train_util.py:4950
2025-10-02 14:56:36 INFO     unet dtype: torch.float32, device: cuda:0                                                 train_network.py:1323
                    INFO     text_encoder [0] dtype: torch.float32, device: cuda:0                                     train_network.py:1329
                    INFO     text_encoder [1] dtype: torch.float32, device: cuda:0                                     train_network.py:1329
import network module: networks.lora
prepare optimizer, data loader etc.
running training / 学習開始
  num train images * repeats / 学習画像の数×繰り返し回数: 344
  num validation images * repeats / 学習画像の数×繰り返し回数: 0
  num reg images / 正則化画像の数: 0
  num batches per epoch / 1epochのバッチ数: 69
  num epochs / epoch数: 29
  batch size per device / バッチサイズ: 5
  gradient accumulation steps / 勾配を合計するステップ数 = 2
  total optimization steps / 学習ステップ数: 1000
steps:   0%|          | 0/1000 [00:00<?, ?it/s]/workspace/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)

epoch 1/29

                    INFO     epoch is incremented. current_epoch: 0, epoch: 1                                              train_util.py:779
                    INFO     epoch is incremented. current_epoch: 0, epoch: 1                                              train_util.py:779
                    INFO     epoch is incremented. current_epoch: 0, epoch: 1                                              train_util.py:779
                    INFO     epoch is incremented. current_epoch: 0, epoch: 1                                              train_util.py:779
steps:   0%|          | 0/1000 [00:04<?, ?it/s, avr_loss=0.0861]steps:   0%|          | 1/1000 [00:08<2:17:53,  8.28s/it, avr_loss=0.0861]steps:   0%|          | 1/1000 [00:08<2:17:53,  8.28s/it, avr_loss=0.0717]steps:   0%|          | 1/1000 [00:11<3:15:20, 11.73s/it, avr_loss=0.0639]steps:   0%|          | 2/1000 [00:15<2:08:05,  7.70s/it, avr_loss=0.0639]steps:   0%|          | 2/1000 [00:15<2:08:05,  7.70s/it, avr_loss=0.0764]steps:   0%|          | 2/1000 [00:18<2:36:53,  9.43s/it, avr_loss=0.0759]steps:   0%|          | 3/1000 [00:22<2:04:50,  7.51s/it, avr_loss=0.0759]steps:   0%|          | 3/1000 [00:22<2:04:50,  7.51s/it, avr_loss=0.0719]steps:   0%|          | 3/1000 [00:26<2:24:02,  8.67s/it, avr_loss=0.0776]steps:   0%|          | 4/1000 [00:29<2:03:12,  7.42s/it, avr_loss=0.0776]steps:   0%|          | 4/1000 [00:29<2:03:12,  7.42s/it, avr_loss=0.087] steps:   0%|          | 4/1000 [00:33<2:17:37,  8.29s/it, avr_loss=0.0859]steps:   0%|          | 5/1000 [00:36<2:02:13,  7.37s/it, avr_loss=0.0859]steps:   0%|          | 5/1000 [00:36<2:02:13,  7.37s/it, avr_loss=0.0816]steps:   0%|          | 5/1000 [00:40<2:13:45,  8.07s/it, avr_loss=0.088] steps:   1%|          | 6/1000 [00:44<2:01:32,  7.34s/it, avr_loss=0.088]steps:   1%|          | 6/1000 [00:44<2:01:32,  7.34s/it, avr_loss=0.0911]steps:   1%|          | 6/1000 [00:47<2:11:09,  7.92s/it, avr_loss=0.0904]steps:   1%|          | 7/1000 [00:51<2:01:03,  7.31s/it, avr_loss=0.0904]steps:   1%|          | 7/1000 [00:51<2:01:03,  7.31s/it, avr_loss=0.0924]steps:   1%|          | 7/1000 [00:54<2:09:18,  7.81s/it, avr_loss=0.0976]steps:   1%|          | 8/1000 [00:58<2:00:41,  7.30s/it, avr_loss=0.0976]steps:   1%|          | 8/1000 [00:58<2:00:41,  7.30s/it, avr_loss=0.0988]steps:   1%|          | 8/1000 [01:01<2:07:54,  7.74s/it, avr_loss=0.101] steps:   1%|          | 9/1000 [01:05<2:00:23,  7.29s/it, avr_loss=0.101]steps:   1%|          | 9/1000 [01:05<2:00:23,  7.29s/it, avr_loss=0.102]steps:   1%|          | 9/1000 [01:09<2:06:48,  7.68s/it, avr_loss=0.0985]steps:   1%|          | 10/1000 [01:12<2:00:09,  7.28s/it, avr_loss=0.0985]steps:   1%|          | 10/1000 [01:12<2:00:09,  7.28s/it, avr_loss=0.0991]steps:   1%|          | 10/1000 [01:16<2:05:55,  7.63s/it, avr_loss=0.102] steps:   1%|          | 11/1000 [01:20<1:59:55,  7.28s/it, avr_loss=0.102]steps:   1%|          | 11/1000 [01:20<1:59:55,  7.28s/it, avr_loss=0.104]steps:   1%|          | 11/1000 [01:23<2:05:10,  7.59s/it, avr_loss=0.103]steps:   1%|          | 12/1000 [01:27<1:59:44,  7.27s/it, avr_loss=0.103]steps:   1%|          | 12/1000 [01:27<1:59:44,  7.27s/it, avr_loss=0.102]steps:   1%|          | 12/1000 [01:30<2:04:32,  7.56s/it, avr_loss=0.0986]steps:   1%|▏         | 13/1000 [01:34<1:59:33,  7.27s/it, avr_loss=0.0986]steps:   1%|▏         | 13/1000 [01:34<1:59:33,  7.27s/it, avr_loss=0.101] steps:   1%|▏         | 13/1000 [01:37<2:04:00,  7.54s/it, avr_loss=0.104]steps:   1%|▏         | 14/1000 [01:41<1:59:23,  7.27s/it, avr_loss=0.104]steps:   1%|▏         | 14/1000 [01:41<1:59:23,  7.27s/it, avr_loss=0.105]steps:   1%|▏         | 14/1000 [01:45<2:03:30,  7.52s/it, avr_loss=0.103]steps:   2%|▏         | 15/1000 [01:48<1:59:14,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 15/1000 [01:48<1:59:14,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 15/1000 [01:52<2:03:05,  7.50s/it, avr_loss=0.104]steps:   2%|▏         | 16/1000 [01:56<1:59:06,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 16/1000 [01:56<1:59:06,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 16/1000 [01:59<2:02:42,  7.48s/it, avr_loss=0.104]steps:   2%|▏         | 17/1000 [02:03<1:58:57,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 17/1000 [02:03<1:58:57,  7.26s/it, avr_loss=0.102]steps:   2%|▏         | 17/1000 [02:06<2:02:21,  7.47s/it, avr_loss=0.103]steps:   2%|▏         | 18/1000 [02:10<1:58:49,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 18/1000 [02:10<1:58:49,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 18/1000 [02:14<2:02:01,  7.46s/it, avr_loss=0.103]steps:   2%|▏         | 19/1000 [02:17<1:58:41,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 19/1000 [02:17<1:58:41,  7.26s/it, avr_loss=0.105]steps:   2%|▏         | 19/1000 [02:21<2:01:43,  7.45s/it, avr_loss=0.106]steps:   2%|▏         | 20/1000 [02:25<1:58:34,  7.26s/it, avr_loss=0.106]steps:   2%|▏         | 20/1000 [02:25<1:58:34,  7.26s/it, avr_loss=0.105]steps:   2%|▏         | 20/1000 [02:28<2:01:26,  7.44s/it, avr_loss=0.105]steps:   2%|▏         | 21/1000 [02:32<1:58:26,  7.26s/it, avr_loss=0.105]steps:   2%|▏         | 21/1000 [02:32<1:58:26,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 21/1000 [02:35<2:01:11,  7.43s/it, avr_loss=0.104]steps:   2%|▏         | 22/1000 [02:39<1:58:19,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 22/1000 [02:39<1:58:19,  7.26s/it, avr_loss=0.104]steps:   2%|▏         | 22/1000 [02:43<2:00:56,  7.42s/it, avr_loss=0.103]steps:   2%|▏         | 23/1000 [02:46<1:58:12,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 23/1000 [02:46<1:58:12,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 23/1000 [02:50<2:00:42,  7.41s/it, avr_loss=0.103]steps:   2%|▏         | 24/1000 [02:54<1:58:05,  7.26s/it, avr_loss=0.103]steps:   2%|▏         | 24/1000 [02:54<1:58:05,  7.26s/it, avr_loss=0.105]steps:   2%|▏         | 24/1000 [02:57<2:00:29,  7.41s/it, avr_loss=0.104]steps:   2%|▎         | 25/1000 [03:01<1:57:58,  7.26s/it, avr_loss=0.104]steps:   2%|▎         | 25/1000 [03:01<1:57:58,  7.26s/it, avr_loss=0.105]steps:   2%|▎         | 25/1000 [03:05<2:00:16,  7.40s/it, avr_loss=0.105]steps:   3%|▎         | 26/1000 [03:08<1:57:51,  7.26s/it, avr_loss=0.105]steps:   3%|▎         | 26/1000 [03:08<1:57:51,  7.26s/it, avr_loss=0.105]steps:   3%|▎         | 26/1000 [03:12<2:00:03,  7.40s/it, avr_loss=0.107]steps:   3%|▎         | 27/1000 [03:16<1:57:44,  7.26s/it, avr_loss=0.107]steps:   3%|▎         | 27/1000 [03:16<1:57:44,  7.26s/it, avr_loss=0.108]steps:   3%|▎         | 27/1000 [03:19<1:59:51,  7.39s/it, avr_loss=0.109]steps:   3%|▎         | 28/1000 [03:23<1:57:37,  7.26s/it, avr_loss=0.109]steps:   3%|▎         | 28/1000 [03:23<1:57:37,  7.26s/it, avr_loss=0.109]steps:   3%|▎         | 28/1000 [03:26<1:59:40,  7.39s/it, avr_loss=0.109]steps:   3%|▎         | 29/1000 [03:30<1:57:31,  7.26s/it, avr_loss=0.109]steps:   3%|▎         | 29/1000 [03:30<1:57:31,  7.26s/it, avr_loss=0.11] steps:   3%|▎         | 29/1000 [03:34<1:59:29,  7.38s/it, avr_loss=0.11]steps:   3%|▎         | 30/1000 [03:37<1:57:24,  7.26s/it, avr_loss=0.11]steps:   3%|▎         | 30/1000 [03:37<1:57:24,  7.26s/it, avr_loss=0.109]steps:   3%|▎         | 30/1000 [03:41<1:59:18,  7.38s/it, avr_loss=0.111]steps:   3%|▎         | 31/1000 [03:45<1:57:17,  7.26s/it, avr_loss=0.111]steps:   3%|▎         | 31/1000 [03:45<1:57:17,  7.26s/it, avr_loss=0.111]steps:   3%|▎         | 31/1000 [03:48<1:59:07,  7.38s/it, avr_loss=0.112]steps:   3%|▎         | 32/1000 [03:51<1:56:45,  7.24s/it, avr_loss=0.112]steps:   3%|▎         | 32/1000 [03:51<1:56:45,  7.24s/it, avr_loss=0.113]steps:   3%|▎         | 32/1000 [03:55<1:58:32,  7.35s/it, avr_loss=0.112]steps:   3%|▎         | 33/1000 [03:58<1:56:39,  7.24s/it, avr_loss=0.112]steps:   3%|▎         | 33/1000 [03:58<1:56:39,  7.24s/it, avr_loss=0.112]steps:   3%|▎         | 33/1000 [04:02<1:58:22,  7.35s/it, avr_loss=0.111]steps:   3%|▎         | 34/1000 [04:06<1:56:33,  7.24s/it, avr_loss=0.111]steps:   3%|▎         | 34/1000 [04:06<1:56:33,  7.24s/it, avr_loss=0.11] steps:   4%|▎         | 35/1000 [04:09<1:54:51,  7.14s/it, avr_loss=0.11]steps:   4%|▎         | 35/1000 [04:09<1:54:51,  7.14s/it, avr_loss=0.11]
epoch 2/29

2025-10-02 15:00:46 INFO     epoch is incremented. current_epoch: 0, epoch: 2                                              train_util.py:779
2025-10-02 15:00:46 INFO     epoch is incremented. current_epoch: 0, epoch: 2                                              train_util.py:779
2025-10-02 15:00:46 INFO     epoch is incremented. current_epoch: 0, epoch: 2                                              train_util.py:779
2025-10-02 15:00:46 INFO     epoch is incremented. current_epoch: 0, epoch: 2                                              train_util.py:779
steps:   4%|▎         | 35/1000 [04:13<1:56:40,  7.25s/it, avr_loss=0.112]steps:   4%|▎         | 36/1000 [04:17<1:54:59,  7.16s/it, avr_loss=0.112]steps:   4%|▎         | 36/1000 [04:17<1:54:59,  7.16s/it, avr_loss=0.113]steps:   4%|▎         | 36/1000 [04:21<1:56:34,  7.26s/it, avr_loss=0.113]steps:   4%|▎         | 37/1000 [04:24<1:54:55,  7.16s/it, avr_loss=0.113]steps:   4%|▎         | 37/1000 [04:24<1:54:55,  7.16s/it, avr_loss=0.113]steps:   4%|▎         | 37/1000 [04:28<1:56:27,  7.26s/it, avr_loss=0.113]steps:   4%|▍         | 38/1000 [04:32<1:54:51,  7.16s/it, avr_loss=0.113]steps:   4%|▍         | 38/1000 [04:32<1:54:51,  7.16s/it, avr_loss=0.115]steps:   4%|▍         | 38/1000 [04:35<1:56:20,  7.26s/it, avr_loss=0.113]steps:   4%|▍         | 39/1000 [04:39<1:54:47,  7.17s/it, avr_loss=0.113]steps:   4%|▍         | 39/1000 [04:39<1:54:47,  7.17s/it, avr_loss=0.114]steps:   4%|▍         | 39/1000 [04:43<1:56:14,  7.26s/it, avr_loss=0.115]steps:   4%|▍         | 40/1000 [04:46<1:54:42,  7.17s/it, avr_loss=0.115]steps:   4%|▍         | 40/1000 [04:46<1:54:42,  7.17s/it, avr_loss=0.115]steps:   4%|▍         | 40/1000 [04:50<1:56:07,  7.26s/it, avr_loss=0.115]steps:   4%|▍         | 41/1000 [04:54<1:54:38,  7.17s/it, avr_loss=0.115]steps:   4%|▍         | 41/1000 [04:54<1:54:38,  7.17s/it, avr_loss=0.115]steps:   4%|▍         | 41/1000 [04:57<1:56:05,  7.26s/it, avr_loss=0.115]steps:   4%|▍         | 42/1000 [05:01<1:54:38,  7.18s/it, avr_loss=0.115]steps:   4%|▍         | 42/1000 [05:01<1:54:38,  7.18s/it, avr_loss=0.115]steps:   4%|▍         | 42/1000 [05:05<1:55:58,  7.26s/it, avr_loss=0.114]steps:   4%|▍         | 43/1000 [05:08<1:54:33,  7.18s/it, avr_loss=0.114]steps:   4%|▍         | 43/1000 [05:08<1:54:33,  7.18s/it, avr_loss=0.114]steps:   4%|▍         | 43/1000 [05:12<1:55:51,  7.26s/it, avr_loss=0.114]steps:   4%|▍         | 44/1000 [05:16<1:54:28,  7.18s/it, avr_loss=0.114]steps:   4%|▍         | 44/1000 [05:16<1:54:28,  7.18s/it, avr_loss=0.113]steps:   4%|▍         | 44/1000 [05:19<1:55:45,  7.26s/it, avr_loss=0.113]steps:   4%|▍         | 45/1000 [05:23<1:54:23,  7.19s/it, avr_loss=0.113]steps:   4%|▍         | 45/1000 [05:23<1:54:23,  7.19s/it, avr_loss=0.114]steps:   4%|▍         | 45/1000 [05:26<1:55:38,  7.27s/it, avr_loss=0.114]steps:   5%|▍         | 46/1000 [05:30<1:54:18,  7.19s/it, avr_loss=0.114]steps:   5%|▍         | 46/1000 [05:30<1:54:18,  7.19s/it, avr_loss=0.113]steps:   5%|▍         | 46/1000 [05:34<1:55:31,  7.27s/it, avr_loss=0.112]steps:   5%|▍         | 47/1000 [05:37<1:54:12,  7.19s/it, avr_loss=0.112]steps:   5%|▍         | 47/1000 [05:37<1:54:12,  7.19s/it, avr_loss=0.113]steps:   5%|▍         | 47/1000 [05:41<1:55:24,  7.27s/it, avr_loss=0.114]steps:   5%|▍         | 48/1000 [05:45<1:54:07,  7.19s/it, avr_loss=0.114]steps:   5%|▍         | 48/1000 [05:45<1:54:07,  7.19s/it, avr_loss=0.114]steps:   5%|▍         | 48/1000 [05:48<1:55:17,  7.27s/it, avr_loss=0.111]steps:   5%|▍         | 49/1000 [05:52<1:54:02,  7.19s/it, avr_loss=0.111]steps:   5%|▍         | 49/1000 [05:52<1:54:02,  7.19s/it, avr_loss=0.111]steps:   5%|▍         | 49/1000 [05:56<1:55:10,  7.27s/it, avr_loss=0.112]steps:   5%|▌         | 50/1000 [05:59<1:53:56,  7.20s/it, avr_loss=0.112]steps:   5%|▌         | 50/1000 [05:59<1:53:56,  7.20s/it, avr_loss=0.11] steps:   5%|▌         | 50/1000 [06:03<1:55:04,  7.27s/it, avr_loss=0.11]steps:   5%|▌         | 51/1000 [06:07<1:53:51,  7.20s/it, avr_loss=0.11]steps:   5%|▌         | 51/1000 [06:07<1:53:51,  7.20s/it, avr_loss=0.111]steps:   5%|▌         | 51/1000 [06:10<1:54:57,  7.27s/it, avr_loss=0.111]steps:   5%|▌         | 52/1000 [06:14<1:53:45,  7.20s/it, avr_loss=0.111]steps:   5%|▌         | 52/1000 [06:14<1:53:45,  7.20s/it, avr_loss=0.112]steps:   5%|▌         | 52/1000 [06:17<1:54:50,  7.27s/it, avr_loss=0.112]steps:   5%|▌         | 53/1000 [06:21<1:53:40,  7.20s/it, avr_loss=0.112]steps:   5%|▌         | 53/1000 [06:21<1:53:40,  7.20s/it, avr_loss=0.112]steps:   5%|▌         | 53/1000 [06:24<1:54:28,  7.25s/it, avr_loss=0.112]steps:   5%|▌         | 54/1000 [06:28<1:53:19,  7.19s/it, avr_loss=0.112]steps:   5%|▌         | 54/1000 [06:28<1:53:19,  7.19s/it, avr_loss=0.111]steps:   5%|▌         | 54/1000 [06:31<1:54:21,  7.25s/it, avr_loss=0.111]steps:   6%|▌         | 55/1000 [06:35<1:53:14,  7.19s/it, avr_loss=0.111]steps:   6%|▌         | 55/1000 [06:35<1:53:14,  7.19s/it, avr_loss=0.11] steps:   6%|▌         | 55/1000 [06:38<1:54:15,  7.25s/it, avr_loss=0.11]steps:   6%|▌         | 56/1000 [06:42<1:53:09,  7.19s/it, avr_loss=0.11]steps:   6%|▌         | 56/1000 [06:42<1:53:09,  7.19s/it, avr_loss=0.111]steps:   6%|▌         | 56/1000 [06:46<1:54:08,  7.25s/it, avr_loss=0.11] steps:   6%|▌         | 57/1000 [06:50<1:53:03,  7.19s/it, avr_loss=0.11]steps:   6%|▌         | 57/1000 [06:50<1:53:03,  7.19s/it, avr_loss=0.111]steps:   6%|▌         | 57/1000 [06:53<1:54:01,  7.26s/it, avr_loss=0.111]steps:   6%|▌         | 58/1000 [06:57<1:52:57,  7.20s/it, avr_loss=0.111]steps:   6%|▌         | 58/1000 [06:57<1:52:57,  7.20s/it, avr_loss=0.111]steps:   6%|▌         | 58/1000 [07:00<1:53:55,  7.26s/it, avr_loss=0.11] steps:   6%|▌         | 59/1000 [07:04<1:52:52,  7.20s/it, avr_loss=0.11]steps:   6%|▌         | 59/1000 [07:04<1:52:52,  7.20s/it, avr_loss=0.11]steps:   6%|▌         | 59/1000 [07:08<1:53:48,  7.26s/it, avr_loss=0.111]steps:   6%|▌         | 60/1000 [07:11<1:52:46,  7.20s/it, avr_loss=0.111]steps:   6%|▌         | 60/1000 [07:11<1:52:46,  7.20s/it, avr_loss=0.112]steps:   6%|▌         | 60/1000 [07:15<1:53:41,  7.26s/it, avr_loss=0.113]steps:   6%|▌         | 61/1000 [07:19<1:52:40,  7.20s/it, avr_loss=0.113]steps:   6%|▌         | 61/1000 [07:19<1:52:40,  7.20s/it, avr_loss=0.113]steps:   6%|▌         | 61/1000 [07:22<1:53:35,  7.26s/it, avr_loss=0.111]steps:   6%|▌         | 62/1000 [07:26<1:52:34,  7.20s/it, avr_loss=0.111]steps:   6%|▌         | 62/1000 [07:26<1:52:34,  7.20s/it, avr_loss=0.111]steps:   6%|▌         | 62/1000 [07:30<1:53:28,  7.26s/it, avr_loss=0.11] steps:   6%|▋         | 63/1000 [07:33<1:52:29,  7.20s/it, avr_loss=0.11]steps:   6%|▋         | 63/1000 [07:33<1:52:29,  7.20s/it, avr_loss=0.111]steps:   6%|▋         | 63/1000 [07:37<1:53:21,  7.26s/it, avr_loss=0.111]steps:   6%|▋         | 64/1000 [07:41<1:52:23,  7.20s/it, avr_loss=0.111]steps:   6%|▋         | 64/1000 [07:41<1:52:23,  7.20s/it, avr_loss=0.109]steps:   6%|▋         | 64/1000 [07:44<1:53:14,  7.26s/it, avr_loss=0.11] steps:   6%|▋         | 65/1000 [07:48<1:52:17,  7.21s/it, avr_loss=0.11]steps:   6%|▋         | 65/1000 [07:48<1:52:17,  7.21s/it, avr_loss=0.11]steps:   6%|▋         | 65/1000 [07:51<1:53:08,  7.26s/it, avr_loss=0.108]steps:   7%|▋         | 66/1000 [07:55<1:52:11,  7.21s/it, avr_loss=0.108]steps:   7%|▋         | 66/1000 [07:55<1:52:11,  7.21s/it, avr_loss=0.107]steps:   7%|▋         | 66/1000 [07:59<1:53:01,  7.26s/it, avr_loss=0.106]steps:   7%|▋         | 67/1000 [08:02<1:52:05,  7.21s/it, avr_loss=0.106]steps:   7%|▋         | 67/1000 [08:02<1:52:05,  7.21s/it, avr_loss=0.106]steps:   7%|▋         | 67/1000 [08:06<1:52:54,  7.26s/it, avr_loss=0.107]steps:   7%|▋         | 68/1000 [08:10<1:51:59,  7.21s/it, avr_loss=0.107]steps:   7%|▋         | 68/1000 [08:10<1:51:59,  7.21s/it, avr_loss=0.108]steps:   7%|▋         | 68/1000 [08:13<1:52:47,  7.26s/it, avr_loss=0.108]steps:   7%|▋         | 69/1000 [08:17<1:51:52,  7.21s/it, avr_loss=0.108]steps:   7%|▋         | 69/1000 [08:17<1:51:52,  7.21s/it, avr_loss=0.109]steps:   7%|▋         | 70/1000 [08:21<1:51:00,  7.16s/it, avr_loss=0.109]steps:   7%|▋         | 70/1000 [08:21<1:51:00,  7.16s/it, avr_loss=0.109]
epoch 3/29

2025-10-02 15:04:58 INFO     epoch is incremented. current_epoch: 0, epoch: 3                                              train_util.py:779
2025-10-02 15:04:58 INFO     epoch is incremented. current_epoch: 0, epoch: 3                                              train_util.py:779
2025-10-02 15:04:58 INFO     epoch is incremented. current_epoch: 0, epoch: 3                                              train_util.py:779
2025-10-02 15:04:58 INFO     epoch is incremented. current_epoch: 0, epoch: 3                                              train_util.py:779
steps:   7%|▋         | 70/1000 [08:25<1:51:53,  7.22s/it, avr_loss=0.108]steps:   7%|▋         | 71/1000 [08:29<1:51:00,  7.17s/it, avr_loss=0.108]steps:   7%|▋         | 71/1000 [08:29<1:51:00,  7.17s/it, avr_loss=0.109]steps:   7%|▋         | 71/1000 [08:32<1:51:46,  7.22s/it, avr_loss=0.109]steps:   7%|▋         | 72/1000 [08:36<1:50:54,  7.17s/it, avr_loss=0.109]steps:   7%|▋         | 72/1000 [08:36<1:50:54,  7.17s/it, avr_loss=0.109]steps:   7%|▋         | 72/1000 [08:39<1:51:40,  7.22s/it, avr_loss=0.109]steps:   7%|▋         | 73/1000 [08:42<1:50:38,  7.16s/it, avr_loss=0.109]steps:   7%|▋         | 73/1000 [08:42<1:50:38,  7.16s/it, avr_loss=0.108]steps:   7%|▋         | 73/1000 [08:46<1:51:23,  7.21s/it, avr_loss=0.11] steps:   7%|▋         | 74/1000 [08:50<1:50:33,  7.16s/it, avr_loss=0.11]steps:   7%|▋         | 74/1000 [08:50<1:50:33,  7.16s/it, avr_loss=0.108]steps:   7%|▋         | 74/1000 [08:53<1:51:17,  7.21s/it, avr_loss=0.107]steps:   8%|▊         | 75/1000 [08:57<1:50:27,  7.16s/it, avr_loss=0.107]steps:   8%|▊         | 75/1000 [08:57<1:50:27,  7.16s/it, avr_loss=0.109]steps:   8%|▊         | 75/1000 [09:00<1:51:11,  7.21s/it, avr_loss=0.108]steps:   8%|▊         | 76/1000 [09:04<1:50:21,  7.17s/it, avr_loss=0.108]steps:   8%|▊         | 76/1000 [09:04<1:50:21,  7.17s/it, avr_loss=0.107]steps:   8%|▊         | 76/1000 [09:08<1:51:04,  7.21s/it, avr_loss=0.107]steps:   8%|▊         | 77/1000 [09:11<1:50:16,  7.17s/it, avr_loss=0.107]steps:   8%|▊         | 77/1000 [09:11<1:50:16,  7.17s/it, avr_loss=0.106]steps:   8%|▊         | 77/1000 [09:15<1:50:58,  7.21s/it, avr_loss=0.107]steps:   8%|▊         | 78/1000 [09:19<1:50:10,  7.17s/it, avr_loss=0.107]steps:   8%|▊         | 78/1000 [09:19<1:50:10,  7.17s/it, avr_loss=0.107]steps:   8%|▊         | 78/1000 [09:22<1:50:52,  7.22s/it, avr_loss=0.106]steps:   8%|▊         | 79/1000 [09:26<1:50:04,  7.17s/it, avr_loss=0.106]steps:   8%|▊         | 79/1000 [09:26<1:50:04,  7.17s/it, avr_loss=0.107]steps:   8%|▊         | 79/1000 [09:30<1:50:45,  7.22s/it, avr_loss=0.109]steps:   8%|▊         | 80/1000 [09:33<1:49:58,  7.17s/it, avr_loss=0.109]steps:   8%|▊         | 80/1000 [09:33<1:49:58,  7.17s/it, avr_loss=0.109]steps:   8%|▊         | 80/1000 [09:37<1:50:39,  7.22s/it, avr_loss=0.109]steps:   8%|▊         | 81/1000 [09:41<1:49:53,  7.17s/it, avr_loss=0.109]steps:   8%|▊         | 81/1000 [09:41<1:49:53,  7.17s/it, avr_loss=0.11] steps:   8%|▊         | 81/1000 [09:44<1:50:33,  7.22s/it, avr_loss=0.11]steps:   8%|▊         | 82/1000 [09:48<1:49:47,  7.18s/it, avr_loss=0.11]steps:   8%|▊         | 82/1000 [09:48<1:49:47,  7.18s/it, avr_loss=0.111]steps:   8%|▊         | 82/1000 [09:51<1:50:26,  7.22s/it, avr_loss=0.11] steps:   8%|▊         | 83/1000 [09:55<1:49:41,  7.18s/it, avr_loss=0.11]steps:   8%|▊         | 83/1000 [09:55<1:49:41,  7.18s/it, avr_loss=0.109]steps:   8%|▊         | 83/1000 [09:59<1:50:20,  7.22s/it, avr_loss=0.11] steps:   8%|▊         | 84/1000 [10:02<1:49:35,  7.18s/it, avr_loss=0.11]steps:   8%|▊         | 84/1000 [10:02<1:49:35,  7.18s/it, avr_loss=0.11]steps:   8%|▊         | 84/1000 [10:06<1:50:13,  7.22s/it, avr_loss=0.109]steps:   8%|▊         | 85/1000 [10:10<1:49:29,  7.18s/it, avr_loss=0.109]steps:   8%|▊         | 85/1000 [10:10<1:49:29,  7.18s/it, avr_loss=0.111]steps:   8%|▊         | 85/1000 [10:13<1:50:07,  7.22s/it, avr_loss=0.111]steps:   9%|▊         | 86/1000 [10:17<1:49:23,  7.18s/it, avr_loss=0.111]steps:   9%|▊         | 86/1000 [10:17<1:49:23,  7.18s/it, avr_loss=0.109]steps:   9%|▊         | 86/1000 [10:21<1:50:00,  7.22s/it, avr_loss=0.11] steps:   9%|▊         | 87/1000 [10:24<1:49:17,  7.18s/it, avr_loss=0.11]steps:   9%|▊         | 87/1000 [10:24<1:49:17,  7.18s/it, avr_loss=0.11]steps:   9%|▊         | 87/1000 [10:28<1:49:54,  7.22s/it, avr_loss=0.11]steps:   9%|▉         | 88/1000 [10:32<1:49:13,  7.19s/it, avr_loss=0.11]steps:   9%|▉         | 88/1000 [10:32<1:49:13,  7.19s/it, avr_loss=0.11]steps:   9%|▉         | 88/1000 [10:35<1:49:49,  7.23s/it, avr_loss=0.111]steps:   9%|▉         | 89/1000 [10:39<1:49:07,  7.19s/it, avr_loss=0.111]steps:   9%|▉         | 89/1000 [10:39<1:49:07,  7.19s/it, avr_loss=0.111]steps:   9%|▉         | 89/1000 [10:43<1:49:43,  7.23s/it, avr_loss=0.11] steps:   9%|▉         | 90/1000 [10:46<1:49:00,  7.19s/it, avr_loss=0.11]steps:   9%|▉         | 90/1000 [10:46<1:49:00,  7.19s/it, avr_loss=0.111]steps:   9%|▉         | 90/1000 [10:50<1:49:36,  7.23s/it, avr_loss=0.11] steps:   9%|▉         | 91/1000 [10:54<1:48:54,  7.19s/it, avr_loss=0.11]steps:   9%|▉         | 91/1000 [10:54<1:48:54,  7.19s/it, avr_loss=0.11]steps:   9%|▉         | 91/1000 [10:57<1:49:30,  7.23s/it, avr_loss=0.111]steps:   9%|▉         | 92/1000 [11:01<1:48:48,  7.19s/it, avr_loss=0.111]steps:   9%|▉         | 92/1000 [11:01<1:48:48,  7.19s/it, avr_loss=0.109]steps:   9%|▉         | 92/1000 [11:05<1:49:23,  7.23s/it, avr_loss=0.109]steps:   9%|▉         | 93/1000 [11:08<1:48:42,  7.19s/it, avr_loss=0.109]steps:   9%|▉         | 93/1000 [11:08<1:48:42,  7.19s/it, avr_loss=0.109]steps:   9%|▉         | 93/1000 [11:12<1:49:16,  7.23s/it, avr_loss=0.109]steps:   9%|▉         | 94/1000 [11:16<1:48:35,  7.19s/it, avr_loss=0.109]steps:   9%|▉         | 94/1000 [11:16<1:48:35,  7.19s/it, avr_loss=0.109]steps:   9%|▉         | 94/1000 [11:19<1:49:10,  7.23s/it, avr_loss=0.108]steps:  10%|▉         | 95/1000 [11:23<1:48:29,  7.19s/it, avr_loss=0.108]steps:  10%|▉         | 95/1000 [11:23<1:48:29,  7.19s/it, avr_loss=0.107]steps:  10%|▉         | 95/1000 [11:26<1:49:03,  7.23s/it, avr_loss=0.106]steps:  10%|▉         | 96/1000 [11:30<1:48:23,  7.19s/it, avr_loss=0.106]steps:  10%|▉         | 96/1000 [11:30<1:48:23,  7.19s/it, avr_loss=0.106]steps:  10%|▉         | 96/1000 [11:34<1:48:56,  7.23s/it, avr_loss=0.107]steps:  10%|▉         | 97/1000 [11:37<1:48:16,  7.19s/it, avr_loss=0.107]steps:  10%|▉         | 97/1000 [11:37<1:48:16,  7.19s/it, avr_loss=0.106]steps:  10%|▉         | 97/1000 [11:41<1:48:49,  7.23s/it, avr_loss=0.106]steps:  10%|▉         | 98/1000 [11:45<1:48:10,  7.20s/it, avr_loss=0.106]steps:  10%|▉         | 98/1000 [11:45<1:48:10,  7.20s/it, avr_loss=0.105]steps:  10%|▉         | 98/1000 [11:48<1:48:43,  7.23s/it, avr_loss=0.105]steps:  10%|▉         | 99/1000 [11:52<1:48:04,  7.20s/it, avr_loss=0.105]steps:  10%|▉         | 99/1000 [11:52<1:48:04,  7.20s/it, avr_loss=0.106]steps:  10%|▉         | 99/1000 [11:56<1:48:36,  7.23s/it, avr_loss=0.104]steps:  10%|█         | 100/1000 [11:59<1:47:57,  7.20s/it, avr_loss=0.104]steps:  10%|█         | 100/1000 [11:59<1:47:57,  7.20s/it, avr_loss=0.103]steps:  10%|█         | 100/1000 [12:03<1:48:29,  7.23s/it, avr_loss=0.103]steps:  10%|█         | 101/1000 [12:07<1:47:51,  7.20s/it, avr_loss=0.103]steps:  10%|█         | 101/1000 [12:07<1:47:51,  7.20s/it, avr_loss=0.105]steps:  10%|█         | 101/1000 [12:10<1:48:22,  7.23s/it, avr_loss=0.103]steps:  10%|█         | 102/1000 [12:14<1:47:44,  7.20s/it, avr_loss=0.103]steps:  10%|█         | 102/1000 [12:14<1:47:44,  7.20s/it, avr_loss=0.102]steps:  10%|█         | 102/1000 [12:17<1:48:15,  7.23s/it, avr_loss=0.102]steps:  10%|█         | 103/1000 [12:21<1:47:38,  7.20s/it, avr_loss=0.102]steps:  10%|█         | 103/1000 [12:21<1:47:38,  7.20s/it, avr_loss=0.101]steps:  10%|█         | 103/1000 [12:25<1:48:09,  7.23s/it, avr_loss=0.102]steps:  10%|█         | 104/1000 [12:28<1:47:31,  7.20s/it, avr_loss=0.102]steps:  10%|█         | 104/1000 [12:28<1:47:31,  7.20s/it, avr_loss=0.101]steps:  10%|█         | 105/1000 [12:32<1:46:55,  7.17s/it, avr_loss=0.101]steps:  10%|█         | 105/1000 [12:32<1:46:55,  7.17s/it, avr_loss=0.0993]
epoch 4/29

2025-10-02 15:09:09 INFO     epoch is incremented. current_epoch: 0, epoch: 4                                 train_util.py:779
2025-10-02 15:09:09 INFO     epoch is incremented. current_epoch: 0, epoch: 4                                 train_util.py:779
2025-10-02 15:09:09 INFO     epoch is incremented. current_epoch: 0, epoch: 4                                 train_util.py:779
2025-10-02 15:09:09 INFO     epoch is incremented. current_epoch: 0, epoch: 4                                 train_util.py:779
steps:  10%|█         | 105/1000 [12:36<1:47:29,  7.21s/it, avr_loss=0.0981]steps:  11%|█         | 106/1000 [12:40<1:46:53,  7.17s/it, avr_loss=0.0981]steps:  11%|█         | 106/1000 [12:40<1:46:53,  7.17s/it, avr_loss=0.0977]steps:  11%|█         | 106/1000 [12:43<1:47:22,  7.21s/it, avr_loss=0.0976]steps:  11%|█         | 107/1000 [12:47<1:46:46,  7.17s/it, avr_loss=0.0976]steps:  11%|█         | 107/1000 [12:47<1:46:46,  7.17s/it, avr_loss=0.0978]steps:  11%|█         | 107/1000 [12:51<1:47:16,  7.21s/it, avr_loss=0.0987]steps:  11%|█         | 108/1000 [12:54<1:46:40,  7.18s/it, avr_loss=0.0987]steps:  11%|█         | 108/1000 [12:54<1:46:40,  7.18s/it, avr_loss=0.0989]steps:  11%|█         | 108/1000 [12:58<1:47:09,  7.21s/it, avr_loss=0.0983]steps:  11%|█         | 109/1000 [13:02<1:46:34,  7.18s/it, avr_loss=0.0983]steps:  11%|█         | 109/1000 [13:02<1:46:34,  7.18s/it, avr_loss=0.0998]steps:  11%|█         | 109/1000 [13:05<1:47:03,  7.21s/it, avr_loss=0.101] steps:  11%|█         | 110/1000 [13:09<1:46:27,  7.18s/it, avr_loss=0.101]steps:  11%|█         | 110/1000 [13:09<1:46:27,  7.18s/it, avr_loss=0.0995]steps:  11%|█         | 110/1000 [13:13<1:46:56,  7.21s/it, avr_loss=0.0992]steps:  11%|█         | 111/1000 [13:16<1:46:21,  7.18s/it, avr_loss=0.0992]steps:  11%|█         | 111/1000 [13:16<1:46:21,  7.18s/it, avr_loss=0.0999]steps:  11%|█         | 111/1000 [13:20<1:46:49,  7.21s/it, avr_loss=0.102] steps:  11%|█         | 112/1000 [13:24<1:46:15,  7.18s/it, avr_loss=0.102]steps:  11%|█         | 112/1000 [13:24<1:46:15,  7.18s/it, avr_loss=0.102]steps:  11%|█         | 112/1000 [13:27<1:46:43,  7.21s/it, avr_loss=0.102]steps:  11%|█▏        | 113/1000 [13:31<1:46:08,  7.18s/it, avr_loss=0.102]steps:  11%|█▏        | 113/1000 [13:31<1:46:08,  7.18s/it, avr_loss=0.102]steps:  11%|█▏        | 113/1000 [13:34<1:46:36,  7.21s/it, avr_loss=0.103]steps:  11%|█▏        | 114/1000 [13:38<1:46:02,  7.18s/it, avr_loss=0.103]steps:  11%|█▏        | 114/1000 [13:38<1:46:02,  7.18s/it, avr_loss=0.103]steps:  11%|█▏        | 114/1000 [13:42<1:46:30,  7.21s/it, avr_loss=0.102]steps:  12%|█▏        | 115/1000 [13:45<1:45:56,  7.18s/it, avr_loss=0.102]steps:  12%|█▏        | 115/1000 [13:45<1:45:56,  7.18s/it, avr_loss=0.102]steps:  12%|█▏        | 115/1000 [13:49<1:46:23,  7.21s/it, avr_loss=0.101]steps:  12%|█▏        | 116/1000 [13:53<1:45:49,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 116/1000 [13:53<1:45:49,  7.18s/it, avr_loss=0.0992]steps:  12%|█▏        | 116/1000 [13:56<1:46:16,  7.21s/it, avr_loss=0.0987]steps:  12%|█▏        | 117/1000 [14:00<1:45:43,  7.18s/it, avr_loss=0.0987]steps:  12%|█▏        | 117/1000 [14:00<1:45:43,  7.18s/it, avr_loss=0.0997]steps:  12%|█▏        | 117/1000 [14:04<1:46:10,  7.21s/it, avr_loss=0.0998]steps:  12%|█▏        | 118/1000 [14:06<1:45:30,  7.18s/it, avr_loss=0.0998]steps:  12%|█▏        | 118/1000 [14:06<1:45:30,  7.18s/it, avr_loss=0.1]   steps:  12%|█▏        | 118/1000 [14:10<1:45:57,  7.21s/it, avr_loss=0.1]steps:  12%|█▏        | 119/1000 [14:14<1:45:24,  7.18s/it, avr_loss=0.1]steps:  12%|█▏        | 119/1000 [14:14<1:45:24,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 119/1000 [14:17<1:45:50,  7.21s/it, avr_loss=0.101]steps:  12%|█▏        | 120/1000 [14:21<1:45:17,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 120/1000 [14:21<1:45:17,  7.18s/it, avr_loss=0.0998]steps:  12%|█▏        | 120/1000 [14:25<1:45:43,  7.21s/it, avr_loss=0.101] steps:  12%|█▏        | 121/1000 [14:28<1:45:11,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 121/1000 [14:28<1:45:11,  7.18s/it, avr_loss=0.102]steps:  12%|█▏        | 121/1000 [14:32<1:45:37,  7.21s/it, avr_loss=0.101]steps:  12%|█▏        | 122/1000 [14:36<1:45:05,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 122/1000 [14:36<1:45:05,  7.18s/it, avr_loss=0.101]steps:  12%|█▏        | 122/1000 [14:39<1:45:30,  7.21s/it, avr_loss=0.0992]steps:  12%|█▏        | 123/1000 [14:43<1:44:58,  7.18s/it, avr_loss=0.0992]steps:  12%|█▏        | 123/1000 [14:43<1:44:58,  7.18s/it, avr_loss=0.0984]steps:  12%|█▏        | 123/1000 [14:46<1:45:23,  7.21s/it, avr_loss=0.0982]steps:  12%|█▏        | 124/1000 [14:50<1:44:52,  7.18s/it, avr_loss=0.0982]steps:  12%|█▏        | 124/1000 [14:50<1:44:52,  7.18s/it, avr_loss=0.0969]steps:  12%|█▏        | 124/1000 [14:54<1:45:17,  7.21s/it, avr_loss=0.0985]steps:  12%|█▎        | 125/1000 [14:57<1:44:45,  7.18s/it, avr_loss=0.0985]steps:  12%|█▎        | 125/1000 [14:57<1:44:45,  7.18s/it, avr_loss=0.0999]steps:  12%|█▎        | 125/1000 [15:01<1:45:10,  7.21s/it, avr_loss=0.1]   steps:  13%|█▎        | 126/1000 [15:05<1:44:39,  7.18s/it, avr_loss=0.1]steps:  13%|█▎        | 126/1000 [15:05<1:44:39,  7.18s/it, avr_loss=0.0995]steps:  13%|█▎        | 126/1000 [15:08<1:45:04,  7.21s/it, avr_loss=0.0997]steps:  13%|█▎        | 127/1000 [15:12<1:44:33,  7.19s/it, avr_loss=0.0997]steps:  13%|█▎        | 127/1000 [15:12<1:44:33,  7.19s/it, avr_loss=0.101] steps:  13%|█▎        | 127/1000 [15:16<1:44:57,  7.21s/it, avr_loss=0.103]steps:  13%|█▎        | 128/1000 [15:19<1:44:26,  7.19s/it, avr_loss=0.103]steps:  13%|█▎        | 128/1000 [15:19<1:44:26,  7.19s/it, avr_loss=0.103]steps:  13%|█▎        | 128/1000 [15:23<1:44:50,  7.21s/it, avr_loss=0.102]steps:  13%|█▎        | 129/1000 [15:27<1:44:20,  7.19s/it, avr_loss=0.102]steps:  13%|█▎        | 129/1000 [15:27<1:44:20,  7.19s/it, avr_loss=0.101]steps:  13%|█▎        | 129/1000 [15:30<1:44:43,  7.21s/it, avr_loss=0.101]steps:  13%|█▎        | 130/1000 [15:34<1:44:13,  7.19s/it, avr_loss=0.101]steps:  13%|█▎        | 130/1000 [15:34<1:44:13,  7.19s/it, avr_loss=0.101]steps:  13%|█▎        | 130/1000 [15:37<1:44:37,  7.22s/it, avr_loss=0.0994]steps:  13%|█▎        | 131/1000 [15:41<1:44:07,  7.19s/it, avr_loss=0.0994]steps:  13%|█▎        | 131/1000 [15:41<1:44:07,  7.19s/it, avr_loss=0.0985]steps:  13%|█▎        | 131/1000 [15:45<1:44:30,  7.22s/it, avr_loss=0.0987]steps:  13%|█▎        | 132/1000 [15:49<1:44:00,  7.19s/it, avr_loss=0.0987]steps:  13%|█▎        | 132/1000 [15:49<1:44:00,  7.19s/it, avr_loss=0.0991]steps:  13%|█▎        | 132/1000 [15:52<1:44:23,  7.22s/it, avr_loss=0.0978]steps:  13%|█▎        | 133/1000 [15:56<1:43:53,  7.19s/it, avr_loss=0.0978]steps:  13%|█▎        | 133/1000 [15:56<1:43:53,  7.19s/it, avr_loss=0.098] steps:  13%|█▎        | 133/1000 [15:59<1:44:17,  7.22s/it, avr_loss=0.0995]steps:  13%|█▎        | 134/1000 [16:03<1:43:47,  7.19s/it, avr_loss=0.0995]steps:  13%|█▎        | 134/1000 [16:03<1:43:47,  7.19s/it, avr_loss=0.0993]steps:  13%|█▎        | 134/1000 [16:07<1:44:10,  7.22s/it, avr_loss=0.0992]steps:  14%|█▎        | 135/1000 [16:10<1:43:40,  7.19s/it, avr_loss=0.0992]steps:  14%|█▎        | 135/1000 [16:10<1:43:40,  7.19s/it, avr_loss=0.103] steps:  14%|█▎        | 135/1000 [16:14<1:44:03,  7.22s/it, avr_loss=0.103]steps:  14%|█▎        | 136/1000 [16:18<1:43:35,  7.19s/it, avr_loss=0.103]steps:  14%|█▎        | 136/1000 [16:18<1:43:35,  7.19s/it, avr_loss=0.102]steps:  14%|█▎        | 136/1000 [16:21<1:43:57,  7.22s/it, avr_loss=0.103]steps:  14%|█▎        | 137/1000 [16:25<1:43:28,  7.19s/it, avr_loss=0.103]steps:  14%|█▎        | 137/1000 [16:25<1:43:28,  7.19s/it, avr_loss=0.104]steps:  14%|█▎        | 137/1000 [16:29<1:43:50,  7.22s/it, avr_loss=0.103]steps:  14%|█▍        | 138/1000 [16:32<1:43:21,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 138/1000 [16:32<1:43:21,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 138/1000 [16:36<1:43:43,  7.22s/it, avr_loss=0.103]steps:  14%|█▍        | 139/1000 [16:40<1:43:15,  7.20s/it, avr_loss=0.103]steps:  14%|█▍        | 139/1000 [16:40<1:43:15,  7.20s/it, avr_loss=0.103]steps:  14%|█▍        | 140/1000 [16:43<1:42:47,  7.17s/it, avr_loss=0.103]steps:  14%|█▍        | 140/1000 [16:43<1:42:47,  7.17s/it, avr_loss=0.103]
epoch 5/29

2025-10-02 15:13:20 INFO     epoch is incremented. current_epoch: 0, epoch: 5                                 train_util.py:779
2025-10-02 15:13:20 INFO     epoch is incremented. current_epoch: 0, epoch: 5                                 train_util.py:779
2025-10-02 15:13:20 INFO     epoch is incremented. current_epoch: 0, epoch: 5                                 train_util.py:779
2025-10-02 15:13:22 INFO     epoch is incremented. current_epoch: 0, epoch: 5                                 train_util.py:779
steps:  14%|█▍        | 140/1000 [16:49<1:43:21,  7.21s/it, avr_loss=0.104]steps:  14%|█▍        | 141/1000 [16:53<1:42:53,  7.19s/it, avr_loss=0.104]steps:  14%|█▍        | 141/1000 [16:53<1:42:53,  7.19s/it, avr_loss=0.104]steps:  14%|█▍        | 141/1000 [16:56<1:43:14,  7.21s/it, avr_loss=0.104]steps:  14%|█▍        | 142/1000 [17:00<1:42:46,  7.19s/it, avr_loss=0.104]steps:  14%|█▍        | 142/1000 [17:00<1:42:46,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 142/1000 [17:04<1:43:07,  7.21s/it, avr_loss=0.103]steps:  14%|█▍        | 143/1000 [17:07<1:42:39,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 143/1000 [17:07<1:42:39,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 143/1000 [17:11<1:43:01,  7.21s/it, avr_loss=0.104]steps:  14%|█▍        | 144/1000 [17:15<1:42:33,  7.19s/it, avr_loss=0.104]steps:  14%|█▍        | 144/1000 [17:15<1:42:33,  7.19s/it, avr_loss=0.102]steps:  14%|█▍        | 144/1000 [17:18<1:42:54,  7.21s/it, avr_loss=0.103]steps:  14%|█▍        | 145/1000 [17:22<1:42:26,  7.19s/it, avr_loss=0.103]steps:  14%|█▍        | 145/1000 [17:22<1:42:26,  7.19s/it, avr_loss=0.102]steps:  14%|█▍        | 145/1000 [17:25<1:42:47,  7.21s/it, avr_loss=0.102]steps:  15%|█▍        | 146/1000 [17:29<1:42:19,  7.19s/it, avr_loss=0.102]steps:  15%|█▍        | 146/1000 [17:29<1:42:19,  7.19s/it, avr_loss=0.103]steps:  15%|█▍        | 146/1000 [17:33<1:42:40,  7.21s/it, avr_loss=0.101]steps:  15%|█▍        | 147/1000 [17:36<1:42:13,  7.19s/it, avr_loss=0.101]steps:  15%|█▍        | 147/1000 [17:36<1:42:13,  7.19s/it, avr_loss=0.1]  steps:  15%|█▍        | 147/1000 [17:40<1:42:33,  7.21s/it, avr_loss=0.101]steps:  15%|█▍        | 148/1000 [17:44<1:42:06,  7.19s/it, avr_loss=0.101]steps:  15%|█▍        | 148/1000 [17:44<1:42:06,  7.19s/it, avr_loss=0.102]steps:  15%|█▍        | 148/1000 [17:47<1:42:26,  7.21s/it, avr_loss=0.101]steps:  15%|█▍        | 149/1000 [17:51<1:41:59,  7.19s/it, avr_loss=0.101]steps:  15%|█▍        | 149/1000 [17:51<1:41:59,  7.19s/it, avr_loss=0.101]steps:  15%|█▍        | 149/1000 [17:55<1:42:20,  7.22s/it, avr_loss=0.101]steps:  15%|█▌        | 150/1000 [17:58<1:41:53,  7.19s/it, avr_loss=0.101]steps:  15%|█▌        | 150/1000 [17:58<1:41:53,  7.19s/it, avr_loss=0.0998]steps:  15%|█▌        | 150/1000 [18:02<1:42:13,  7.22s/it, avr_loss=0.0995]steps:  15%|█▌        | 151/1000 [18:06<1:41:46,  7.19s/it, avr_loss=0.0995]steps:  15%|█▌        | 151/1000 [18:06<1:41:46,  7.19s/it, avr_loss=0.0999]steps:  15%|█▌        | 151/1000 [18:09<1:42:06,  7.22s/it, avr_loss=0.1]   steps:  15%|█▌        | 152/1000 [18:13<1:41:39,  7.19s/it, avr_loss=0.1]steps:  15%|█▌        | 152/1000 [18:13<1:41:39,  7.19s/it, avr_loss=0.0987]steps:  15%|█▌        | 152/1000 [18:16<1:41:59,  7.22s/it, avr_loss=0.0993]steps:  15%|█▌        | 153/1000 [18:20<1:41:33,  7.19s/it, avr_loss=0.0993]steps:  15%|█▌        | 153/1000 [18:20<1:41:33,  7.19s/it, avr_loss=0.0999]steps:  15%|█▌        | 153/1000 [18:24<1:41:52,  7.22s/it, avr_loss=0.0995]steps:  15%|█▌        | 154/1000 [18:27<1:41:26,  7.19s/it, avr_loss=0.0995]steps:  15%|█▌        | 154/1000 [18:27<1:41:26,  7.19s/it, avr_loss=0.0984]steps:  15%|█▌        | 154/1000 [18:31<1:41:45,  7.22s/it, avr_loss=0.1]   steps:  16%|█▌        | 155/1000 [18:35<1:41:19,  7.20s/it, avr_loss=0.1]steps:  16%|█▌        | 155/1000 [18:35<1:41:19,  7.20s/it, avr_loss=0.1]steps:  16%|█▌        | 155/1000 [18:38<1:41:39,  7.22s/it, avr_loss=0.0999]steps:  16%|█▌        | 156/1000 [18:42<1:41:13,  7.20s/it, avr_loss=0.0999]steps:  16%|█▌        | 156/1000 [18:42<1:41:13,  7.20s/it, avr_loss=0.1]   steps:  16%|█▌        | 156/1000 [18:46<1:41:32,  7.22s/it, avr_loss=0.102]steps:  16%|█▌        | 157/1000 [18:49<1:41:06,  7.20s/it, avr_loss=0.102]steps:  16%|█▌        | 157/1000 [18:49<1:41:06,  7.20s/it, avr_loss=0.102]steps:  16%|█▌        | 157/1000 [18:53<1:41:25,  7.22s/it, avr_loss=0.103]steps:  16%|█▌        | 158/1000 [18:57<1:40:59,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 158/1000 [18:57<1:40:59,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 158/1000 [19:00<1:41:18,  7.22s/it, avr_loss=0.102]steps:  16%|█▌        | 159/1000 [19:04<1:40:52,  7.20s/it, avr_loss=0.102]steps:  16%|█▌        | 159/1000 [19:04<1:40:52,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 159/1000 [19:07<1:41:11,  7.22s/it, avr_loss=0.103]steps:  16%|█▌        | 160/1000 [19:11<1:40:46,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 160/1000 [19:11<1:40:46,  7.20s/it, avr_loss=0.101]steps:  16%|█▌        | 160/1000 [19:15<1:41:04,  7.22s/it, avr_loss=0.101]steps:  16%|█▌        | 161/1000 [19:18<1:40:39,  7.20s/it, avr_loss=0.101]steps:  16%|█▌        | 161/1000 [19:18<1:40:39,  7.20s/it, avr_loss=0.102]steps:  16%|█▌        | 161/1000 [19:22<1:40:57,  7.22s/it, avr_loss=0.103]steps:  16%|█▌        | 162/1000 [19:26<1:40:32,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 162/1000 [19:26<1:40:32,  7.20s/it, avr_loss=0.103]steps:  16%|█▌        | 162/1000 [19:29<1:40:50,  7.22s/it, avr_loss=0.103]steps:  16%|█▋        | 163/1000 [19:33<1:40:25,  7.20s/it, avr_loss=0.103]steps:  16%|█▋        | 163/1000 [19:33<1:40:25,  7.20s/it, avr_loss=0.102]steps:  16%|█▋        | 163/1000 [19:37<1:40:43,  7.22s/it, avr_loss=0.102]steps:  16%|█▋        | 164/1000 [19:40<1:40:19,  7.20s/it, avr_loss=0.102]steps:  16%|█▋        | 164/1000 [19:40<1:40:19,  7.20s/it, avr_loss=0.103]steps:  16%|█▋        | 164/1000 [19:44<1:40:37,  7.22s/it, avr_loss=0.103]steps:  16%|█▋        | 165/1000 [19:48<1:40:12,  7.20s/it, avr_loss=0.103]steps:  16%|█▋        | 165/1000 [19:48<1:40:12,  7.20s/it, avr_loss=0.103]steps:  16%|█▋        | 165/1000 [19:50<1:40:25,  7.22s/it, avr_loss=0.103]steps:  17%|█▋        | 166/1000 [19:54<1:40:01,  7.20s/it, avr_loss=0.103]steps:  17%|█▋        | 166/1000 [19:54<1:40:01,  7.20s/it, avr_loss=0.104]steps:  17%|█▋        | 166/1000 [19:58<1:40:18,  7.22s/it, avr_loss=0.104]steps:  17%|█▋        | 167/1000 [20:01<1:39:54,  7.20s/it, avr_loss=0.104]steps:  17%|█▋        | 167/1000 [20:01<1:39:54,  7.20s/it, avr_loss=0.103]steps:  17%|█▋        | 167/1000 [20:05<1:40:12,  7.22s/it, avr_loss=0.104]steps:  17%|█▋        | 168/1000 [20:09<1:39:47,  7.20s/it, avr_loss=0.104]steps:  17%|█▋        | 168/1000 [20:09<1:39:47,  7.20s/it, avr_loss=0.103]steps:  17%|█▋        | 168/1000 [20:12<1:40:05,  7.22s/it, avr_loss=0.102]steps:  17%|█▋        | 169/1000 [20:16<1:39:40,  7.20s/it, avr_loss=0.102]steps:  17%|█▋        | 169/1000 [20:16<1:39:40,  7.20s/it, avr_loss=0.101]steps:  17%|█▋        | 169/1000 [20:19<1:39:58,  7.22s/it, avr_loss=0.103]steps:  17%|█▋        | 170/1000 [20:23<1:39:34,  7.20s/it, avr_loss=0.103]steps:  17%|█▋        | 170/1000 [20:23<1:39:34,  7.20s/it, avr_loss=0.1]  steps:  17%|█▋        | 170/1000 [20:27<1:39:51,  7.22s/it, avr_loss=0.0999]steps:  17%|█▋        | 171/1000 [20:30<1:39:27,  7.20s/it, avr_loss=0.0999]steps:  17%|█▋        | 171/1000 [20:30<1:39:27,  7.20s/it, avr_loss=0.0996]steps:  17%|█▋        | 171/1000 [20:34<1:39:44,  7.22s/it, avr_loss=0.1]   steps:  17%|█▋        | 172/1000 [20:38<1:39:20,  7.20s/it, avr_loss=0.1]steps:  17%|█▋        | 172/1000 [20:38<1:39:20,  7.20s/it, avr_loss=0.1]steps:  17%|█▋        | 172/1000 [20:41<1:39:37,  7.22s/it, avr_loss=0.1]steps:  17%|█▋        | 173/1000 [20:45<1:39:13,  7.20s/it, avr_loss=0.1]steps:  17%|█▋        | 173/1000 [20:45<1:39:13,  7.20s/it, avr_loss=0.1]steps:  17%|█▋        | 173/1000 [20:48<1:39:30,  7.22s/it, avr_loss=0.101]steps:  17%|█▋        | 174/1000 [20:52<1:39:06,  7.20s/it, avr_loss=0.101]steps:  17%|█▋        | 174/1000 [20:52<1:39:06,  7.20s/it, avr_loss=0.101]steps:  18%|█▊        | 175/1000 [20:56<1:38:43,  7.18s/it, avr_loss=0.101]steps:  18%|█▊        | 175/1000 [20:56<1:38:43,  7.18s/it, avr_loss=0.101]
epoch 6/29

2025-10-02 15:17:33 INFO     epoch is incremented. current_epoch: 0, epoch: 6                                 train_util.py:779
2025-10-02 15:17:33 INFO     epoch is incremented. current_epoch: 0, epoch: 6                                 train_util.py:779
2025-10-02 15:17:33 INFO     epoch is incremented. current_epoch: 0, epoch: 6                                 train_util.py:779
2025-10-02 15:17:33 INFO     epoch is incremented. current_epoch: 0, epoch: 6                                 train_util.py:779
steps:  18%|█▊        | 175/1000 [21:00<1:39:02,  7.20s/it, avr_loss=0.101]steps:  18%|█▊        | 176/1000 [21:04<1:38:38,  7.18s/it, avr_loss=0.101]steps:  18%|█▊        | 176/1000 [21:04<1:38:38,  7.18s/it, avr_loss=0.102]steps:  18%|█▊        | 176/1000 [21:07<1:38:55,  7.20s/it, avr_loss=0.101]steps:  18%|█▊        | 177/1000 [21:11<1:38:32,  7.18s/it, avr_loss=0.101]steps:  18%|█▊        | 177/1000 [21:11<1:38:32,  7.18s/it, avr_loss=0.101]steps:  18%|█▊        | 177/1000 [21:15<1:38:48,  7.20s/it, avr_loss=0.101]steps:  18%|█▊        | 178/1000 [21:18<1:38:25,  7.18s/it, avr_loss=0.101]steps:  18%|█▊        | 178/1000 [21:18<1:38:25,  7.18s/it, avr_loss=0.1]  steps:  18%|█▊        | 178/1000 [21:22<1:38:41,  7.20s/it, avr_loss=0.0995]steps:  18%|█▊        | 179/1000 [21:26<1:38:18,  7.18s/it, avr_loss=0.0995]steps:  18%|█▊        | 179/1000 [21:26<1:38:18,  7.18s/it, avr_loss=0.0991]steps:  18%|█▊        | 179/1000 [21:29<1:38:34,  7.20s/it, avr_loss=0.0974]steps:  18%|█▊        | 180/1000 [21:33<1:38:11,  7.19s/it, avr_loss=0.0974]steps:  18%|█▊        | 180/1000 [21:33<1:38:11,  7.19s/it, avr_loss=0.0992]steps:  18%|█▊        | 180/1000 [21:36<1:38:27,  7.20s/it, avr_loss=0.099] steps:  18%|█▊        | 181/1000 [21:40<1:38:04,  7.19s/it, avr_loss=0.099]steps:  18%|█▊        | 181/1000 [21:40<1:38:04,  7.19s/it, avr_loss=0.0983]steps:  18%|█▊        | 181/1000 [21:44<1:38:20,  7.21s/it, avr_loss=0.0988]steps:  18%|█▊        | 182/1000 [21:47<1:37:58,  7.19s/it, avr_loss=0.0988]steps:  18%|█▊        | 182/1000 [21:47<1:37:58,  7.19s/it, avr_loss=0.0987]steps:  18%|█▊        | 182/1000 [21:51<1:38:14,  7.21s/it, avr_loss=0.0969]steps:  18%|█▊        | 183/1000 [21:55<1:37:51,  7.19s/it, avr_loss=0.0969]steps:  18%|█▊        | 183/1000 [21:55<1:37:51,  7.19s/it, avr_loss=0.097] steps:  18%|█▊        | 183/1000 [21:58<1:38:07,  7.21s/it, avr_loss=0.0965]steps:  18%|█▊        | 184/1000 [22:02<1:37:45,  7.19s/it, avr_loss=0.0965]steps:  18%|█▊        | 184/1000 [22:02<1:37:45,  7.19s/it, avr_loss=0.0964]steps:  18%|█▊        | 184/1000 [22:06<1:38:00,  7.21s/it, avr_loss=0.096] steps:  18%|█▊        | 185/1000 [22:09<1:37:38,  7.19s/it, avr_loss=0.096]steps:  18%|█▊        | 185/1000 [22:09<1:37:38,  7.19s/it, avr_loss=0.0965]steps:  18%|█▊        | 185/1000 [22:13<1:37:54,  7.21s/it, avr_loss=0.0982]steps:  19%|█▊        | 186/1000 [22:17<1:37:31,  7.19s/it, avr_loss=0.0982]steps:  19%|█▊        | 186/1000 [22:17<1:37:31,  7.19s/it, avr_loss=0.0992]steps:  19%|█▊        | 186/1000 [22:20<1:37:47,  7.21s/it, avr_loss=0.1]   steps:  19%|█▊        | 187/1000 [22:24<1:37:25,  7.19s/it, avr_loss=0.1]steps:  19%|█▊        | 187/1000 [22:24<1:37:25,  7.19s/it, avr_loss=0.1]steps:  19%|█▊        | 187/1000 [22:27<1:37:40,  7.21s/it, avr_loss=0.0998]steps:  19%|█▉        | 188/1000 [22:31<1:37:18,  7.19s/it, avr_loss=0.0998]steps:  19%|█▉        | 188/1000 [22:31<1:37:18,  7.19s/it, avr_loss=0.098] steps:  19%|█▉        | 188/1000 [22:35<1:37:33,  7.21s/it, avr_loss=0.0986]steps:  19%|█▉        | 189/1000 [22:38<1:37:11,  7.19s/it, avr_loss=0.0986]steps:  19%|█▉        | 189/1000 [22:38<1:37:11,  7.19s/it, avr_loss=0.1]   steps:  19%|█▉        | 189/1000 [22:42<1:37:26,  7.21s/it, avr_loss=0.0996]steps:  19%|█▉        | 190/1000 [22:46<1:37:04,  7.19s/it, avr_loss=0.0996]steps:  19%|█▉        | 190/1000 [22:46<1:37:04,  7.19s/it, avr_loss=0.1]   steps:  19%|█▉        | 190/1000 [22:49<1:37:19,  7.21s/it, avr_loss=0.1]steps:  19%|█▉        | 191/1000 [22:53<1:36:57,  7.19s/it, avr_loss=0.1]steps:  19%|█▉        | 191/1000 [22:53<1:36:57,  7.19s/it, avr_loss=0.101]steps:  19%|█▉        | 191/1000 [22:56<1:37:09,  7.21s/it, avr_loss=0.0989]steps:  19%|█▉        | 192/1000 [23:00<1:36:47,  7.19s/it, avr_loss=0.0989]steps:  19%|█▉        | 192/1000 [23:00<1:36:47,  7.19s/it, avr_loss=0.0995]steps:  19%|█▉        | 192/1000 [23:03<1:37:02,  7.21s/it, avr_loss=0.0999]steps:  19%|█▉        | 193/1000 [23:07<1:36:40,  7.19s/it, avr_loss=0.0999]steps:  19%|█▉        | 193/1000 [23:07<1:36:40,  7.19s/it, avr_loss=0.101] steps:  19%|█▉        | 193/1000 [23:10<1:36:55,  7.21s/it, avr_loss=0.102]steps:  19%|█▉        | 194/1000 [23:14<1:36:33,  7.19s/it, avr_loss=0.102]steps:  19%|█▉        | 194/1000 [23:14<1:36:33,  7.19s/it, avr_loss=0.104]steps:  19%|█▉        | 194/1000 [23:18<1:36:48,  7.21s/it, avr_loss=0.102]steps:  20%|█▉        | 195/1000 [23:21<1:36:27,  7.19s/it, avr_loss=0.102]steps:  20%|█▉        | 195/1000 [23:21<1:36:27,  7.19s/it, avr_loss=0.102]steps:  20%|█▉        | 195/1000 [23:25<1:36:41,  7.21s/it, avr_loss=0.102]steps:  20%|█▉        | 196/1000 [23:29<1:36:20,  7.19s/it, avr_loss=0.102]steps:  20%|█▉        | 196/1000 [23:29<1:36:20,  7.19s/it, avr_loss=0.101]steps:  20%|█▉        | 196/1000 [23:32<1:36:34,  7.21s/it, avr_loss=0.0997]steps:  20%|█▉        | 197/1000 [23:36<1:36:13,  7.19s/it, avr_loss=0.0997]steps:  20%|█▉        | 197/1000 [23:36<1:36:13,  7.19s/it, avr_loss=0.1]   steps:  20%|█▉        | 197/1000 [23:39<1:36:28,  7.21s/it, avr_loss=0.0996]steps:  20%|█▉        | 198/1000 [23:43<1:36:06,  7.19s/it, avr_loss=0.0996]steps:  20%|█▉        | 198/1000 [23:43<1:36:06,  7.19s/it, avr_loss=0.101] steps:  20%|█▉        | 198/1000 [23:47<1:36:21,  7.21s/it, avr_loss=0.102]steps:  20%|█▉        | 199/1000 [23:51<1:36:00,  7.19s/it, avr_loss=0.102]steps:  20%|█▉        | 199/1000 [23:51<1:36:00,  7.19s/it, avr_loss=0.101]steps:  20%|█▉        | 199/1000 [23:54<1:36:14,  7.21s/it, avr_loss=0.103]steps:  20%|██        | 200/1000 [23:58<1:35:53,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 200/1000 [23:58<1:35:53,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 200/1000 [24:01<1:36:07,  7.21s/it, avr_loss=0.103]steps:  20%|██        | 201/1000 [24:05<1:35:46,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 201/1000 [24:05<1:35:46,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 201/1000 [24:09<1:36:00,  7.21s/it, avr_loss=0.102]steps:  20%|██        | 202/1000 [24:12<1:35:39,  7.19s/it, avr_loss=0.102]steps:  20%|██        | 202/1000 [24:12<1:35:39,  7.19s/it, avr_loss=0.102]steps:  20%|██        | 202/1000 [24:16<1:35:53,  7.21s/it, avr_loss=0.103]steps:  20%|██        | 203/1000 [24:20<1:35:32,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 203/1000 [24:20<1:35:32,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 203/1000 [24:23<1:35:46,  7.21s/it, avr_loss=0.103]steps:  20%|██        | 204/1000 [24:27<1:35:25,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 204/1000 [24:27<1:35:25,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 204/1000 [24:30<1:35:39,  7.21s/it, avr_loss=0.103]steps:  20%|██        | 205/1000 [24:34<1:35:19,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 205/1000 [24:34<1:35:19,  7.19s/it, avr_loss=0.103]steps:  20%|██        | 205/1000 [24:38<1:35:32,  7.21s/it, avr_loss=0.104]steps:  21%|██        | 206/1000 [24:42<1:35:12,  7.19s/it, avr_loss=0.104]steps:  21%|██        | 206/1000 [24:42<1:35:12,  7.19s/it, avr_loss=0.104]steps:  21%|██        | 206/1000 [24:45<1:35:25,  7.21s/it, avr_loss=0.102]steps:  21%|██        | 207/1000 [24:49<1:35:05,  7.19s/it, avr_loss=0.102]steps:  21%|██        | 207/1000 [24:49<1:35:05,  7.19s/it, avr_loss=0.103]steps:  21%|██        | 207/1000 [24:52<1:35:19,  7.21s/it, avr_loss=0.103]steps:  21%|██        | 208/1000 [24:56<1:34:58,  7.20s/it, avr_loss=0.103]steps:  21%|██        | 208/1000 [24:56<1:34:58,  7.20s/it, avr_loss=0.104]steps:  21%|██        | 208/1000 [25:00<1:35:12,  7.21s/it, avr_loss=0.102]steps:  21%|██        | 209/1000 [25:03<1:34:51,  7.20s/it, avr_loss=0.102]steps:  21%|██        | 209/1000 [25:03<1:34:51,  7.20s/it, avr_loss=0.103]steps:  21%|██        | 210/1000 [25:07<1:34:31,  7.18s/it, avr_loss=0.103]steps:  21%|██        | 210/1000 [25:07<1:34:31,  7.18s/it, avr_loss=0.104]
epoch 7/29

2025-10-02 15:21:44 INFO     epoch is incremented. current_epoch: 0, epoch: 7                                 train_util.py:779
2025-10-02 15:21:44 INFO     epoch is incremented. current_epoch: 0, epoch: 7                                 train_util.py:779
2025-10-02 15:21:44 INFO     epoch is incremented. current_epoch: 0, epoch: 7                                 train_util.py:779
2025-10-02 15:21:46 INFO     epoch is incremented. current_epoch: 0, epoch: 7                                 train_util.py:779
steps:  21%|██        | 210/1000 [25:11<1:34:46,  7.20s/it, avr_loss=0.104]steps:  21%|██        | 211/1000 [25:15<1:34:26,  7.18s/it, avr_loss=0.104]steps:  21%|██        | 211/1000 [25:15<1:34:26,  7.18s/it, avr_loss=0.103]steps:  21%|██        | 211/1000 [25:18<1:34:39,  7.20s/it, avr_loss=0.103]steps:  21%|██        | 212/1000 [25:22<1:34:19,  7.18s/it, avr_loss=0.103]steps:  21%|██        | 212/1000 [25:22<1:34:19,  7.18s/it, avr_loss=0.104]steps:  21%|██        | 212/1000 [25:25<1:34:29,  7.20s/it, avr_loss=0.106]steps:  21%|██▏       | 213/1000 [25:29<1:34:09,  7.18s/it, avr_loss=0.106]steps:  21%|██▏       | 213/1000 [25:29<1:34:09,  7.18s/it, avr_loss=0.106]steps:  21%|██▏       | 213/1000 [25:32<1:34:22,  7.20s/it, avr_loss=0.105]steps:  21%|██▏       | 214/1000 [25:36<1:34:03,  7.18s/it, avr_loss=0.105]steps:  21%|██▏       | 214/1000 [25:36<1:34:03,  7.18s/it, avr_loss=0.107]steps:  21%|██▏       | 214/1000 [25:39<1:34:16,  7.20s/it, avr_loss=0.107]steps:  22%|██▏       | 215/1000 [25:43<1:33:56,  7.18s/it, avr_loss=0.107]steps:  22%|██▏       | 215/1000 [25:43<1:33:56,  7.18s/it, avr_loss=0.106]steps:  22%|██▏       | 215/1000 [25:47<1:34:09,  7.20s/it, avr_loss=0.107]steps:  22%|██▏       | 216/1000 [25:50<1:33:49,  7.18s/it, avr_loss=0.107]steps:  22%|██▏       | 216/1000 [25:50<1:33:49,  7.18s/it, avr_loss=0.107]steps:  22%|██▏       | 216/1000 [25:54<1:34:02,  7.20s/it, avr_loss=0.108]steps:  22%|██▏       | 217/1000 [25:58<1:33:42,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 217/1000 [25:58<1:33:42,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 217/1000 [26:01<1:33:55,  7.20s/it, avr_loss=0.11] steps:  22%|██▏       | 218/1000 [26:05<1:33:35,  7.18s/it, avr_loss=0.11]steps:  22%|██▏       | 218/1000 [26:05<1:33:35,  7.18s/it, avr_loss=0.11]steps:  22%|██▏       | 218/1000 [26:09<1:33:48,  7.20s/it, avr_loss=0.11]steps:  22%|██▏       | 219/1000 [26:12<1:33:29,  7.18s/it, avr_loss=0.11]steps:  22%|██▏       | 219/1000 [26:12<1:33:29,  7.18s/it, avr_loss=0.11]steps:  22%|██▏       | 219/1000 [26:16<1:33:41,  7.20s/it, avr_loss=0.111]steps:  22%|██▏       | 220/1000 [26:20<1:33:22,  7.18s/it, avr_loss=0.111]steps:  22%|██▏       | 220/1000 [26:20<1:33:22,  7.18s/it, avr_loss=0.11] steps:  22%|██▏       | 220/1000 [26:23<1:33:34,  7.20s/it, avr_loss=0.108]steps:  22%|██▏       | 221/1000 [26:27<1:33:15,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 221/1000 [26:27<1:33:15,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 221/1000 [26:30<1:33:28,  7.20s/it, avr_loss=0.107]steps:  22%|██▏       | 222/1000 [26:34<1:33:08,  7.18s/it, avr_loss=0.107]steps:  22%|██▏       | 222/1000 [26:34<1:33:08,  7.18s/it, avr_loss=0.106]steps:  22%|██▏       | 222/1000 [26:38<1:33:21,  7.20s/it, avr_loss=0.107]steps:  22%|██▏       | 223/1000 [26:42<1:33:01,  7.18s/it, avr_loss=0.107]steps:  22%|██▏       | 223/1000 [26:42<1:33:01,  7.18s/it, avr_loss=0.106]steps:  22%|██▏       | 223/1000 [26:45<1:33:14,  7.20s/it, avr_loss=0.108]steps:  22%|██▏       | 224/1000 [26:49<1:32:55,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 224/1000 [26:49<1:32:55,  7.18s/it, avr_loss=0.108]steps:  22%|██▏       | 224/1000 [26:52<1:33:07,  7.20s/it, avr_loss=0.107]steps:  22%|██▎       | 225/1000 [26:56<1:32:48,  7.18s/it, avr_loss=0.107]steps:  22%|██▎       | 225/1000 [26:56<1:32:48,  7.18s/it, avr_loss=0.107]steps:  22%|██▎       | 225/1000 [27:00<1:33:00,  7.20s/it, avr_loss=0.107]steps:  23%|██▎       | 226/1000 [27:03<1:32:41,  7.19s/it, avr_loss=0.107]steps:  23%|██▎       | 226/1000 [27:03<1:32:41,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 226/1000 [27:07<1:32:53,  7.20s/it, avr_loss=0.106]steps:  23%|██▎       | 227/1000 [27:11<1:32:34,  7.19s/it, avr_loss=0.106]steps:  23%|██▎       | 227/1000 [27:11<1:32:34,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 227/1000 [27:14<1:32:46,  7.20s/it, avr_loss=0.105]steps:  23%|██▎       | 228/1000 [27:18<1:32:27,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 228/1000 [27:18<1:32:27,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 228/1000 [27:21<1:32:39,  7.20s/it, avr_loss=0.104]steps:  23%|██▎       | 229/1000 [27:25<1:32:20,  7.19s/it, avr_loss=0.104]steps:  23%|██▎       | 229/1000 [27:25<1:32:20,  7.19s/it, avr_loss=0.103]steps:  23%|██▎       | 229/1000 [27:29<1:32:32,  7.20s/it, avr_loss=0.104]steps:  23%|██▎       | 230/1000 [27:33<1:32:14,  7.19s/it, avr_loss=0.104]steps:  23%|██▎       | 230/1000 [27:33<1:32:14,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 230/1000 [27:36<1:32:25,  7.20s/it, avr_loss=0.106]steps:  23%|██▎       | 231/1000 [27:40<1:32:07,  7.19s/it, avr_loss=0.106]steps:  23%|██▎       | 231/1000 [27:40<1:32:07,  7.19s/it, avr_loss=0.107]steps:  23%|██▎       | 231/1000 [27:43<1:32:19,  7.20s/it, avr_loss=0.107]steps:  23%|██▎       | 232/1000 [27:47<1:32:00,  7.19s/it, avr_loss=0.107]steps:  23%|██▎       | 232/1000 [27:47<1:32:00,  7.19s/it, avr_loss=0.106]steps:  23%|██▎       | 232/1000 [27:51<1:32:12,  7.20s/it, avr_loss=0.105]steps:  23%|██▎       | 233/1000 [27:55<1:31:53,  7.19s/it, avr_loss=0.105]steps:  23%|██▎       | 233/1000 [27:55<1:31:53,  7.19s/it, avr_loss=0.103]steps:  23%|██▎       | 233/1000 [27:58<1:32:05,  7.20s/it, avr_loss=0.103]steps:  23%|██▎       | 234/1000 [28:02<1:31:47,  7.19s/it, avr_loss=0.103]steps:  23%|██▎       | 234/1000 [28:02<1:31:47,  7.19s/it, avr_loss=0.104]steps:  23%|██▎       | 234/1000 [28:05<1:31:58,  7.20s/it, avr_loss=0.103]steps:  24%|██▎       | 235/1000 [28:09<1:31:40,  7.19s/it, avr_loss=0.103]steps:  24%|██▎       | 235/1000 [28:09<1:31:40,  7.19s/it, avr_loss=0.103]steps:  24%|██▎       | 235/1000 [28:13<1:31:51,  7.20s/it, avr_loss=0.104]steps:  24%|██▎       | 236/1000 [28:16<1:31:33,  7.19s/it, avr_loss=0.104]steps:  24%|██▎       | 236/1000 [28:16<1:31:33,  7.19s/it, avr_loss=0.104]steps:  24%|██▎       | 236/1000 [28:20<1:31:44,  7.21s/it, avr_loss=0.104]steps:  24%|██▎       | 237/1000 [28:24<1:31:26,  7.19s/it, avr_loss=0.104]steps:  24%|██▎       | 237/1000 [28:24<1:31:26,  7.19s/it, avr_loss=0.104]steps:  24%|██▎       | 237/1000 [28:27<1:31:37,  7.21s/it, avr_loss=0.103]steps:  24%|██▍       | 238/1000 [28:31<1:31:19,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 238/1000 [28:31<1:31:19,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 238/1000 [28:35<1:31:31,  7.21s/it, avr_loss=0.103]steps:  24%|██▍       | 239/1000 [28:38<1:31:12,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 239/1000 [28:38<1:31:12,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 239/1000 [28:42<1:31:24,  7.21s/it, avr_loss=0.104]steps:  24%|██▍       | 240/1000 [28:46<1:31:05,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 240/1000 [28:46<1:31:05,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 240/1000 [28:49<1:31:17,  7.21s/it, avr_loss=0.103]steps:  24%|██▍       | 241/1000 [28:53<1:30:59,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 241/1000 [28:53<1:30:59,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 241/1000 [28:56<1:31:10,  7.21s/it, avr_loss=0.104]steps:  24%|██▍       | 242/1000 [29:00<1:30:52,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 242/1000 [29:00<1:30:52,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 242/1000 [29:04<1:31:03,  7.21s/it, avr_loss=0.102]steps:  24%|██▍       | 243/1000 [29:07<1:30:45,  7.19s/it, avr_loss=0.102]steps:  24%|██▍       | 243/1000 [29:07<1:30:45,  7.19s/it, avr_loss=0.102]steps:  24%|██▍       | 243/1000 [29:11<1:30:56,  7.21s/it, avr_loss=0.103]steps:  24%|██▍       | 244/1000 [29:15<1:30:38,  7.19s/it, avr_loss=0.103]steps:  24%|██▍       | 244/1000 [29:15<1:30:38,  7.19s/it, avr_loss=0.104]steps:  24%|██▍       | 245/1000 [29:19<1:30:20,  7.18s/it, avr_loss=0.104]steps:  24%|██▍       | 245/1000 [29:19<1:30:20,  7.18s/it, avr_loss=0.102]
epoch 8/29

2025-10-02 15:25:55 INFO     epoch is incremented. current_epoch: 0, epoch: 8                                 train_util.py:779
2025-10-02 15:25:55 INFO     epoch is incremented. current_epoch: 0, epoch: 8                                 train_util.py:779
2025-10-02 15:25:55 INFO     epoch is incremented. current_epoch: 0, epoch: 8                                 train_util.py:779
2025-10-02 15:25:55 INFO     epoch is incremented. current_epoch: 0, epoch: 8                                 train_util.py:779
steps:  24%|██▍       | 245/1000 [29:23<1:30:32,  7.20s/it, avr_loss=0.103]steps:  25%|██▍       | 246/1000 [29:26<1:30:15,  7.18s/it, avr_loss=0.103]steps:  25%|██▍       | 246/1000 [29:26<1:30:15,  7.18s/it, avr_loss=0.103]steps:  25%|██▍       | 246/1000 [29:30<1:30:26,  7.20s/it, avr_loss=0.104]steps:  25%|██▍       | 247/1000 [29:34<1:30:08,  7.18s/it, avr_loss=0.104]steps:  25%|██▍       | 247/1000 [29:34<1:30:08,  7.18s/it, avr_loss=0.105]steps:  25%|██▍       | 247/1000 [29:37<1:30:19,  7.20s/it, avr_loss=0.104]steps:  25%|██▍       | 248/1000 [29:41<1:30:01,  7.18s/it, avr_loss=0.104]steps:  25%|██▍       | 248/1000 [29:41<1:30:01,  7.18s/it, avr_loss=0.105]steps:  25%|██▍       | 248/1000 [29:44<1:30:12,  7.20s/it, avr_loss=0.106]steps:  25%|██▍       | 249/1000 [29:48<1:29:54,  7.18s/it, avr_loss=0.106]steps:  25%|██▍       | 249/1000 [29:48<1:29:54,  7.18s/it, avr_loss=0.104]steps:  25%|██▍       | 249/1000 [29:52<1:30:05,  7.20s/it, avr_loss=0.105]steps:  25%|██▌       | 250/1000 [29:55<1:29:47,  7.18s/it, avr_loss=0.105]steps:  25%|██▌       | 250/1000 [29:55<1:29:47,  7.18s/it, avr_loss=0.105]steps:  25%|██▌       | 250/1000 [29:59<1:29:58,  7.20s/it, avr_loss=0.103]steps:  25%|██▌       | 251/1000 [30:03<1:29:40,  7.18s/it, avr_loss=0.103]steps:  25%|██▌       | 251/1000 [30:03<1:29:40,  7.18s/it, avr_loss=0.103]steps:  25%|██▌       | 251/1000 [30:06<1:29:51,  7.20s/it, avr_loss=0.102]steps:  25%|██▌       | 252/1000 [30:10<1:29:33,  7.18s/it, avr_loss=0.102]steps:  25%|██▌       | 252/1000 [30:10<1:29:33,  7.18s/it, avr_loss=0.101]steps:  25%|██▌       | 252/1000 [30:14<1:29:44,  7.20s/it, avr_loss=0.102]steps:  25%|██▌       | 253/1000 [30:17<1:29:27,  7.18s/it, avr_loss=0.102]steps:  25%|██▌       | 253/1000 [30:17<1:29:27,  7.18s/it, avr_loss=0.103]steps:  25%|██▌       | 253/1000 [30:21<1:29:37,  7.20s/it, avr_loss=0.102]steps:  25%|██▌       | 254/1000 [30:25<1:29:20,  7.19s/it, avr_loss=0.102]steps:  25%|██▌       | 254/1000 [30:25<1:29:20,  7.19s/it, avr_loss=0.102]steps:  25%|██▌       | 254/1000 [30:28<1:29:30,  7.20s/it, avr_loss=0.101]steps:  26%|██▌       | 255/1000 [30:32<1:29:13,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 255/1000 [30:32<1:29:13,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 255/1000 [30:35<1:29:23,  7.20s/it, avr_loss=0.102]steps:  26%|██▌       | 256/1000 [30:39<1:29:06,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 256/1000 [30:39<1:29:06,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 256/1000 [30:43<1:29:16,  7.20s/it, avr_loss=0.101]steps:  26%|██▌       | 257/1000 [30:46<1:28:59,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 257/1000 [30:46<1:28:59,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 257/1000 [30:50<1:29:09,  7.20s/it, avr_loss=0.102]steps:  26%|██▌       | 258/1000 [30:54<1:28:52,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 258/1000 [30:54<1:28:52,  7.19s/it, avr_loss=0.103]steps:  26%|██▌       | 258/1000 [30:57<1:29:02,  7.20s/it, avr_loss=0.102]steps:  26%|██▌       | 259/1000 [31:01<1:28:45,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 259/1000 [31:01<1:28:45,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 259/1000 [31:05<1:28:55,  7.20s/it, avr_loss=0.101]steps:  26%|██▌       | 260/1000 [31:08<1:28:38,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 260/1000 [31:08<1:28:38,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 260/1000 [31:12<1:28:49,  7.20s/it, avr_loss=0.101]steps:  26%|██▌       | 261/1000 [31:16<1:28:32,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 261/1000 [31:16<1:28:32,  7.19s/it, avr_loss=0.101]steps:  26%|██▌       | 261/1000 [31:19<1:28:42,  7.20s/it, avr_loss=0.102]steps:  26%|██▌       | 262/1000 [31:23<1:28:25,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 262/1000 [31:23<1:28:25,  7.19s/it, avr_loss=0.102]steps:  26%|██▌       | 262/1000 [31:26<1:28:35,  7.20s/it, avr_loss=0.101]steps:  26%|██▋       | 263/1000 [31:30<1:28:18,  7.19s/it, avr_loss=0.101]steps:  26%|██▋       | 263/1000 [31:30<1:28:18,  7.19s/it, avr_loss=0.102]steps:  26%|██▋       | 263/1000 [31:34<1:28:28,  7.20s/it, avr_loss=0.1]  steps:  26%|██▋       | 264/1000 [31:37<1:28:11,  7.19s/it, avr_loss=0.1]steps:  26%|██▋       | 264/1000 [31:37<1:28:11,  7.19s/it, avr_loss=0.0994]steps:  26%|██▋       | 264/1000 [31:41<1:28:21,  7.20s/it, avr_loss=0.0984]steps:  26%|██▋       | 265/1000 [31:45<1:28:04,  7.19s/it, avr_loss=0.0984]steps:  26%|██▋       | 265/1000 [31:45<1:28:04,  7.19s/it, avr_loss=0.0971]steps:  26%|██▋       | 265/1000 [31:48<1:28:14,  7.20s/it, avr_loss=0.0986]steps:  27%|██▋       | 266/1000 [31:52<1:27:57,  7.19s/it, avr_loss=0.0986]steps:  27%|██▋       | 266/1000 [31:52<1:27:57,  7.19s/it, avr_loss=0.0983]steps:  27%|██▋       | 266/1000 [31:56<1:28:07,  7.20s/it, avr_loss=0.0994]steps:  27%|██▋       | 267/1000 [31:59<1:27:50,  7.19s/it, avr_loss=0.0994]steps:  27%|██▋       | 267/1000 [31:59<1:27:50,  7.19s/it, avr_loss=0.0994]steps:  27%|██▋       | 267/1000 [32:03<1:28:00,  7.20s/it, avr_loss=0.1]   steps:  27%|██▋       | 268/1000 [32:07<1:27:43,  7.19s/it, avr_loss=0.1]steps:  27%|██▋       | 268/1000 [32:07<1:27:43,  7.19s/it, avr_loss=0.101]steps:  27%|██▋       | 268/1000 [32:10<1:27:53,  7.20s/it, avr_loss=0.0998]steps:  27%|██▋       | 269/1000 [32:14<1:27:36,  7.19s/it, avr_loss=0.0998]steps:  27%|██▋       | 269/1000 [32:14<1:27:36,  7.19s/it, avr_loss=0.099] steps:  27%|██▋       | 269/1000 [32:17<1:27:46,  7.20s/it, avr_loss=0.1]  steps:  27%|██▋       | 270/1000 [32:21<1:27:29,  7.19s/it, avr_loss=0.1]steps:  27%|██▋       | 270/1000 [32:21<1:27:29,  7.19s/it, avr_loss=0.101]steps:  27%|██▋       | 270/1000 [32:24<1:27:37,  7.20s/it, avr_loss=0.1]  steps:  27%|██▋       | 271/1000 [32:28<1:27:20,  7.19s/it, avr_loss=0.1]steps:  27%|██▋       | 271/1000 [32:28<1:27:20,  7.19s/it, avr_loss=0.0999]steps:  27%|██▋       | 271/1000 [32:31<1:27:30,  7.20s/it, avr_loss=0.0989]steps:  27%|██▋       | 272/1000 [32:35<1:27:13,  7.19s/it, avr_loss=0.0989]steps:  27%|██▋       | 272/1000 [32:35<1:27:13,  7.19s/it, avr_loss=0.0995]steps:  27%|██▋       | 272/1000 [32:38<1:27:23,  7.20s/it, avr_loss=0.0998]steps:  27%|██▋       | 273/1000 [32:42<1:27:06,  7.19s/it, avr_loss=0.0998]steps:  27%|██▋       | 273/1000 [32:42<1:27:06,  7.19s/it, avr_loss=0.102] steps:  27%|██▋       | 273/1000 [32:46<1:27:16,  7.20s/it, avr_loss=0.101]steps:  27%|██▋       | 274/1000 [32:49<1:26:59,  7.19s/it, avr_loss=0.101]steps:  27%|██▋       | 274/1000 [32:50<1:26:59,  7.19s/it, avr_loss=0.101]steps:  27%|██▋       | 274/1000 [32:53<1:27:09,  7.20s/it, avr_loss=0.102]steps:  28%|██▊       | 275/1000 [32:57<1:26:52,  7.19s/it, avr_loss=0.102]steps:  28%|██▊       | 275/1000 [32:57<1:26:52,  7.19s/it, avr_loss=0.102]steps:  28%|██▊       | 275/1000 [33:00<1:27:02,  7.20s/it, avr_loss=0.103]steps:  28%|██▊       | 276/1000 [33:04<1:26:45,  7.19s/it, avr_loss=0.103]steps:  28%|██▊       | 276/1000 [33:04<1:26:45,  7.19s/it, avr_loss=0.104]steps:  28%|██▊       | 276/1000 [33:08<1:26:55,  7.20s/it, avr_loss=0.104]steps:  28%|██▊       | 277/1000 [33:11<1:26:38,  7.19s/it, avr_loss=0.104]steps:  28%|██▊       | 277/1000 [33:11<1:26:38,  7.19s/it, avr_loss=0.105]steps:  28%|██▊       | 277/1000 [33:15<1:26:48,  7.20s/it, avr_loss=0.106]steps:  28%|██▊       | 278/1000 [33:19<1:26:32,  7.19s/it, avr_loss=0.106]steps:  28%|██▊       | 278/1000 [33:19<1:26:32,  7.19s/it, avr_loss=0.105]steps:  28%|██▊       | 278/1000 [33:22<1:26:41,  7.20s/it, avr_loss=0.104]steps:  28%|██▊       | 279/1000 [33:26<1:26:25,  7.19s/it, avr_loss=0.104]steps:  28%|██▊       | 279/1000 [33:26<1:26:25,  7.19s/it, avr_loss=0.105]steps:  28%|██▊       | 280/1000 [33:30<1:26:09,  7.18s/it, avr_loss=0.105]steps:  28%|██▊       | 280/1000 [33:30<1:26:09,  7.18s/it, avr_loss=0.107]
epoch 9/29

2025-10-02 15:30:07 INFO     epoch is incremented. current_epoch: 0, epoch: 9                                 train_util.py:779
2025-10-02 15:30:07 INFO     epoch is incremented. current_epoch: 0, epoch: 9                                 train_util.py:779
2025-10-02 15:30:07 INFO     epoch is incremented. current_epoch: 0, epoch: 9                                 train_util.py:779
2025-10-02 15:30:07 INFO     epoch is incremented. current_epoch: 0, epoch: 9                                 train_util.py:779
steps:  28%|██▊       | 280/1000 [33:34<1:26:19,  7.19s/it, avr_loss=0.107]steps:  28%|██▊       | 281/1000 [33:38<1:26:03,  7.18s/it, avr_loss=0.107]steps:  28%|██▊       | 281/1000 [33:38<1:26:03,  7.18s/it, avr_loss=0.108]steps:  28%|██▊       | 281/1000 [33:41<1:26:12,  7.19s/it, avr_loss=0.107]steps:  28%|██▊       | 282/1000 [33:45<1:25:56,  7.18s/it, avr_loss=0.107]steps:  28%|██▊       | 282/1000 [33:45<1:25:56,  7.18s/it, avr_loss=0.106]steps:  28%|██▊       | 282/1000 [33:48<1:26:03,  7.19s/it, avr_loss=0.105]steps:  28%|██▊       | 283/1000 [33:51<1:25:47,  7.18s/it, avr_loss=0.105]steps:  28%|██▊       | 283/1000 [33:51<1:25:47,  7.18s/it, avr_loss=0.103]steps:  28%|██▊       | 283/1000 [33:55<1:25:56,  7.19s/it, avr_loss=0.105]steps:  28%|██▊       | 284/1000 [33:59<1:25:40,  7.18s/it, avr_loss=0.105]steps:  28%|██▊       | 284/1000 [33:59<1:25:40,  7.18s/it, avr_loss=0.105]steps:  28%|██▊       | 284/1000 [34:02<1:25:49,  7.19s/it, avr_loss=0.104]steps:  28%|██▊       | 285/1000 [34:06<1:25:33,  7.18s/it, avr_loss=0.104]steps:  28%|██▊       | 285/1000 [34:06<1:25:33,  7.18s/it, avr_loss=0.104]steps:  28%|██▊       | 285/1000 [34:09<1:25:42,  7.19s/it, avr_loss=0.104]steps:  29%|██▊       | 286/1000 [34:13<1:25:26,  7.18s/it, avr_loss=0.104]steps:  29%|██▊       | 286/1000 [34:13<1:25:26,  7.18s/it, avr_loss=0.104]steps:  29%|██▊       | 286/1000 [34:17<1:25:35,  7.19s/it, avr_loss=0.105]steps:  29%|██▊       | 287/1000 [34:20<1:25:19,  7.18s/it, avr_loss=0.105]steps:  29%|██▊       | 287/1000 [34:20<1:25:19,  7.18s/it, avr_loss=0.105]steps:  29%|██▊       | 287/1000 [34:24<1:25:28,  7.19s/it, avr_loss=0.103]steps:  29%|██▉       | 288/1000 [34:28<1:25:13,  7.18s/it, avr_loss=0.103]steps:  29%|██▉       | 288/1000 [34:28<1:25:13,  7.18s/it, avr_loss=0.104]steps:  29%|██▉       | 288/1000 [34:31<1:25:21,  7.19s/it, avr_loss=0.103]steps:  29%|██▉       | 289/1000 [34:35<1:25:06,  7.18s/it, avr_loss=0.103]steps:  29%|██▉       | 289/1000 [34:35<1:25:06,  7.18s/it, avr_loss=0.104]steps:  29%|██▉       | 289/1000 [34:39<1:25:14,  7.19s/it, avr_loss=0.103]steps:  29%|██▉       | 290/1000 [34:42<1:24:59,  7.18s/it, avr_loss=0.103]steps:  29%|██▉       | 290/1000 [34:42<1:24:59,  7.18s/it, avr_loss=0.102]steps:  29%|██▉       | 290/1000 [34:46<1:25:07,  7.19s/it, avr_loss=0.101]steps:  29%|██▉       | 291/1000 [34:50<1:24:52,  7.18s/it, avr_loss=0.101]steps:  29%|██▉       | 291/1000 [34:50<1:24:52,  7.18s/it, avr_loss=0.102]steps:  29%|██▉       | 291/1000 [34:53<1:25:00,  7.19s/it, avr_loss=0.102]steps:  29%|██▉       | 292/1000 [34:57<1:24:45,  7.18s/it, avr_loss=0.102]steps:  29%|██▉       | 292/1000 [34:57<1:24:45,  7.18s/it, avr_loss=0.101]steps:  29%|██▉       | 292/1000 [35:00<1:24:53,  7.19s/it, avr_loss=0.102]steps:  29%|██▉       | 293/1000 [35:04<1:24:38,  7.18s/it, avr_loss=0.102]steps:  29%|██▉       | 293/1000 [35:04<1:24:38,  7.18s/it, avr_loss=0.101]steps:  29%|██▉       | 293/1000 [35:08<1:24:46,  7.20s/it, avr_loss=0.101]steps:  29%|██▉       | 294/1000 [35:11<1:24:31,  7.18s/it, avr_loss=0.101]steps:  29%|██▉       | 294/1000 [35:11<1:24:31,  7.18s/it, avr_loss=0.101]steps:  29%|██▉       | 294/1000 [35:15<1:24:39,  7.20s/it, avr_loss=0.102]steps:  30%|██▉       | 295/1000 [35:19<1:24:24,  7.18s/it, avr_loss=0.102]steps:  30%|██▉       | 295/1000 [35:19<1:24:24,  7.18s/it, avr_loss=0.103]steps:  30%|██▉       | 295/1000 [35:22<1:24:33,  7.20s/it, avr_loss=0.103]steps:  30%|██▉       | 296/1000 [35:26<1:24:17,  7.18s/it, avr_loss=0.103]steps:  30%|██▉       | 296/1000 [35:26<1:24:17,  7.18s/it, avr_loss=0.103]steps:  30%|██▉       | 296/1000 [35:30<1:24:26,  7.20s/it, avr_loss=0.102]steps:  30%|██▉       | 297/1000 [35:33<1:24:10,  7.18s/it, avr_loss=0.102]steps:  30%|██▉       | 297/1000 [35:33<1:24:10,  7.18s/it, avr_loss=0.102]steps:  30%|██▉       | 297/1000 [35:37<1:24:19,  7.20s/it, avr_loss=0.102]steps:  30%|██▉       | 298/1000 [35:41<1:24:03,  7.18s/it, avr_loss=0.102]steps:  30%|██▉       | 298/1000 [35:41<1:24:03,  7.18s/it, avr_loss=0.102]steps:  30%|██▉       | 298/1000 [35:44<1:24:12,  7.20s/it, avr_loss=0.102]steps:  30%|██▉       | 299/1000 [35:48<1:23:56,  7.19s/it, avr_loss=0.102]steps:  30%|██▉       | 299/1000 [35:48<1:23:56,  7.19s/it, avr_loss=0.104]steps:  30%|██▉       | 299/1000 [35:51<1:24:05,  7.20s/it, avr_loss=0.105]steps:  30%|███       | 300/1000 [35:55<1:23:49,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 300/1000 [35:55<1:23:49,  7.19s/it, avr_loss=0.106]steps:  30%|███       | 300/1000 [35:59<1:23:58,  7.20s/it, avr_loss=0.104]steps:  30%|███       | 301/1000 [36:02<1:23:42,  7.19s/it, avr_loss=0.104]steps:  30%|███       | 301/1000 [36:02<1:23:42,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 301/1000 [36:06<1:23:51,  7.20s/it, avr_loss=0.105]steps:  30%|███       | 302/1000 [36:10<1:23:35,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 302/1000 [36:10<1:23:35,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 302/1000 [36:13<1:23:44,  7.20s/it, avr_loss=0.105]steps:  30%|███       | 303/1000 [36:17<1:23:28,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 303/1000 [36:17<1:23:28,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 303/1000 [36:21<1:23:37,  7.20s/it, avr_loss=0.106]steps:  30%|███       | 304/1000 [36:24<1:23:21,  7.19s/it, avr_loss=0.106]steps:  30%|███       | 304/1000 [36:24<1:23:21,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 304/1000 [36:28<1:23:30,  7.20s/it, avr_loss=0.106]steps:  30%|███       | 305/1000 [36:32<1:23:15,  7.19s/it, avr_loss=0.106]steps:  30%|███       | 305/1000 [36:32<1:23:15,  7.19s/it, avr_loss=0.105]steps:  30%|███       | 305/1000 [36:35<1:23:23,  7.20s/it, avr_loss=0.104]steps:  31%|███       | 306/1000 [36:39<1:23:08,  7.19s/it, avr_loss=0.104]steps:  31%|███       | 306/1000 [36:39<1:23:08,  7.19s/it, avr_loss=0.105]steps:  31%|███       | 306/1000 [36:42<1:23:16,  7.20s/it, avr_loss=0.106]steps:  31%|███       | 307/1000 [36:46<1:23:01,  7.19s/it, avr_loss=0.106]steps:  31%|███       | 307/1000 [36:46<1:23:01,  7.19s/it, avr_loss=0.105]steps:  31%|███       | 307/1000 [36:50<1:23:09,  7.20s/it, avr_loss=0.106]steps:  31%|███       | 308/1000 [36:53<1:22:54,  7.19s/it, avr_loss=0.106]steps:  31%|███       | 308/1000 [36:53<1:22:54,  7.19s/it, avr_loss=0.105]steps:  31%|███       | 308/1000 [36:57<1:23:02,  7.20s/it, avr_loss=0.105]steps:  31%|███       | 309/1000 [37:01<1:22:47,  7.19s/it, avr_loss=0.105]steps:  31%|███       | 309/1000 [37:01<1:22:47,  7.19s/it, avr_loss=0.104]steps:  31%|███       | 309/1000 [37:04<1:22:55,  7.20s/it, avr_loss=0.103]steps:  31%|███       | 310/1000 [37:08<1:22:40,  7.19s/it, avr_loss=0.103]steps:  31%|███       | 310/1000 [37:08<1:22:40,  7.19s/it, avr_loss=0.104]steps:  31%|███       | 310/1000 [37:12<1:22:48,  7.20s/it, avr_loss=0.103]steps:  31%|███       | 311/1000 [37:15<1:22:33,  7.19s/it, avr_loss=0.103]steps:  31%|███       | 311/1000 [37:15<1:22:33,  7.19s/it, avr_loss=0.102]steps:  31%|███       | 311/1000 [37:19<1:22:41,  7.20s/it, avr_loss=0.102]steps:  31%|███       | 312/1000 [37:23<1:22:26,  7.19s/it, avr_loss=0.102]steps:  31%|███       | 312/1000 [37:23<1:22:26,  7.19s/it, avr_loss=0.102]steps:  31%|███       | 312/1000 [37:26<1:22:34,  7.20s/it, avr_loss=0.101]steps:  31%|███▏      | 313/1000 [37:30<1:22:19,  7.19s/it, avr_loss=0.101]steps:  31%|███▏      | 313/1000 [37:30<1:22:19,  7.19s/it, avr_loss=0.101]steps:  31%|███▏      | 313/1000 [37:33<1:22:27,  7.20s/it, avr_loss=0.102]steps:  31%|███▏      | 314/1000 [37:37<1:22:12,  7.19s/it, avr_loss=0.102]steps:  31%|███▏      | 314/1000 [37:37<1:22:12,  7.19s/it, avr_loss=0.101]steps:  32%|███▏      | 315/1000 [37:41<1:21:57,  7.18s/it, avr_loss=0.101]steps:  32%|███▏      | 315/1000 [37:41<1:21:57,  7.18s/it, avr_loss=0.1]  
epoch 10/29

2025-10-02 15:34:18 INFO     epoch is incremented. current_epoch: 0, epoch: 10                                train_util.py:779
2025-10-02 15:34:18 INFO     epoch is incremented. current_epoch: 0, epoch: 10                                train_util.py:779
2025-10-02 15:34:18 INFO     epoch is incremented. current_epoch: 0, epoch: 10                                train_util.py:779
2025-10-02 15:34:18 INFO     epoch is incremented. current_epoch: 0, epoch: 10                                train_util.py:779
steps:  32%|███▏      | 315/1000 [37:45<1:22:06,  7.19s/it, avr_loss=0.1]steps:  32%|███▏      | 316/1000 [37:49<1:21:51,  7.18s/it, avr_loss=0.1]steps:  32%|███▏      | 316/1000 [37:49<1:21:51,  7.18s/it, avr_loss=0.101]steps:  32%|███▏      | 316/1000 [37:52<1:21:59,  7.19s/it, avr_loss=0.1]  steps:  32%|███▏      | 317/1000 [37:56<1:21:44,  7.18s/it, avr_loss=0.1]steps:  32%|███▏      | 317/1000 [37:56<1:21:44,  7.18s/it, avr_loss=0.0994]steps:  32%|███▏      | 317/1000 [37:59<1:21:52,  7.19s/it, avr_loss=0.0995]steps:  32%|███▏      | 318/1000 [38:03<1:21:37,  7.18s/it, avr_loss=0.0995]steps:  32%|███▏      | 318/1000 [38:03<1:21:37,  7.18s/it, avr_loss=0.0999]steps:  32%|███▏      | 318/1000 [38:07<1:21:45,  7.19s/it, avr_loss=0.0974]steps:  32%|███▏      | 319/1000 [38:10<1:21:30,  7.18s/it, avr_loss=0.0974]steps:  32%|███▏      | 319/1000 [38:10<1:21:30,  7.18s/it, avr_loss=0.0975]steps:  32%|███▏      | 319/1000 [38:13<1:21:36,  7.19s/it, avr_loss=0.0979]steps:  32%|███▏      | 320/1000 [38:17<1:21:21,  7.18s/it, avr_loss=0.0979]steps:  32%|███▏      | 320/1000 [38:17<1:21:21,  7.18s/it, avr_loss=0.0996]steps:  32%|███▏      | 320/1000 [38:20<1:21:29,  7.19s/it, avr_loss=0.101] steps:  32%|███▏      | 321/1000 [38:24<1:21:14,  7.18s/it, avr_loss=0.101]steps:  32%|███▏      | 321/1000 [38:24<1:21:14,  7.18s/it, avr_loss=0.102]steps:  32%|███▏      | 321/1000 [38:28<1:21:22,  7.19s/it, avr_loss=0.102]steps:  32%|███▏      | 322/1000 [38:31<1:21:07,  7.18s/it, avr_loss=0.102]steps:  32%|███▏      | 322/1000 [38:31<1:21:07,  7.18s/it, avr_loss=0.101]steps:  32%|███▏      | 322/1000 [38:35<1:21:15,  7.19s/it, avr_loss=0.101]steps:  32%|███▏      | 323/1000 [38:39<1:21:00,  7.18s/it, avr_loss=0.101]steps:  32%|███▏      | 323/1000 [38:39<1:21:00,  7.18s/it, avr_loss=0.0998]steps:  32%|███▏      | 323/1000 [38:42<1:21:08,  7.19s/it, avr_loss=0.1]   steps:  32%|███▏      | 324/1000 [38:46<1:20:53,  7.18s/it, avr_loss=0.1]steps:  32%|███▏      | 324/1000 [38:46<1:20:53,  7.18s/it, avr_loss=0.1]steps:  32%|███▏      | 324/1000 [38:49<1:21:01,  7.19s/it, avr_loss=0.101]steps:  32%|███▎      | 325/1000 [38:53<1:20:47,  7.18s/it, avr_loss=0.101]steps:  32%|███▎      | 325/1000 [38:53<1:20:47,  7.18s/it, avr_loss=0.102]steps:  32%|███▎      | 325/1000 [38:57<1:20:54,  7.19s/it, avr_loss=0.103]steps:  33%|███▎      | 326/1000 [39:01<1:20:40,  7.18s/it, avr_loss=0.103]steps:  33%|███▎      | 326/1000 [39:01<1:20:40,  7.18s/it, avr_loss=0.104]steps:  33%|███▎      | 326/1000 [39:04<1:20:47,  7.19s/it, avr_loss=0.103]steps:  33%|███▎      | 327/1000 [39:08<1:20:33,  7.18s/it, avr_loss=0.103]steps:  33%|███▎      | 327/1000 [39:08<1:20:33,  7.18s/it, avr_loss=0.104]steps:  33%|███▎      | 327/1000 [39:11<1:20:40,  7.19s/it, avr_loss=0.105]steps:  33%|███▎      | 328/1000 [39:15<1:20:26,  7.18s/it, avr_loss=0.105]steps:  33%|███▎      | 328/1000 [39:15<1:20:26,  7.18s/it, avr_loss=0.105]steps:  33%|███▎      | 328/1000 [39:19<1:20:33,  7.19s/it, avr_loss=0.104]steps:  33%|███▎      | 329/1000 [39:22<1:20:19,  7.18s/it, avr_loss=0.104]steps:  33%|███▎      | 329/1000 [39:22<1:20:19,  7.18s/it, avr_loss=0.104]steps:  33%|███▎      | 329/1000 [39:26<1:20:26,  7.19s/it, avr_loss=0.104]steps:  33%|███▎      | 330/1000 [39:30<1:20:12,  7.18s/it, avr_loss=0.104]steps:  33%|███▎      | 330/1000 [39:30<1:20:12,  7.18s/it, avr_loss=0.102]steps:  33%|███▎      | 330/1000 [39:33<1:20:19,  7.19s/it, avr_loss=0.101]steps:  33%|███▎      | 331/1000 [39:37<1:20:05,  7.18s/it, avr_loss=0.101]steps:  33%|███▎      | 331/1000 [39:37<1:20:05,  7.18s/it, avr_loss=0.101]steps:  33%|███▎      | 331/1000 [39:41<1:20:12,  7.19s/it, avr_loss=0.101]steps:  33%|███▎      | 332/1000 [39:44<1:19:58,  7.18s/it, avr_loss=0.101]steps:  33%|███▎      | 332/1000 [39:44<1:19:58,  7.18s/it, avr_loss=0.102]steps:  33%|███▎      | 332/1000 [39:48<1:20:05,  7.19s/it, avr_loss=0.102]steps:  33%|███▎      | 333/1000 [39:52<1:19:51,  7.18s/it, avr_loss=0.102]steps:  33%|███▎      | 333/1000 [39:52<1:19:51,  7.18s/it, avr_loss=0.101]steps:  33%|███▎      | 333/1000 [39:55<1:19:58,  7.19s/it, avr_loss=0.101]steps:  33%|███▎      | 334/1000 [39:59<1:19:44,  7.18s/it, avr_loss=0.101]steps:  33%|███▎      | 334/1000 [39:59<1:19:44,  7.18s/it, avr_loss=0.1]  steps:  33%|███▎      | 334/1000 [40:02<1:19:51,  7.19s/it, avr_loss=0.0993]steps:  34%|███▎      | 335/1000 [40:06<1:19:37,  7.18s/it, avr_loss=0.0993]steps:  34%|███▎      | 335/1000 [40:06<1:19:37,  7.18s/it, avr_loss=0.0984]steps:  34%|███▎      | 335/1000 [40:10<1:19:44,  7.19s/it, avr_loss=0.0992]steps:  34%|███▎      | 336/1000 [40:13<1:19:30,  7.18s/it, avr_loss=0.0992]steps:  34%|███▎      | 336/1000 [40:13<1:19:30,  7.18s/it, avr_loss=0.098] steps:  34%|███▎      | 336/1000 [40:17<1:19:37,  7.19s/it, avr_loss=0.0967]steps:  34%|███▎      | 337/1000 [40:21<1:19:23,  7.18s/it, avr_loss=0.0967]steps:  34%|███▎      | 337/1000 [40:21<1:19:23,  7.18s/it, avr_loss=0.0978]steps:  34%|███▎      | 337/1000 [40:24<1:19:30,  7.19s/it, avr_loss=0.0971]steps:  34%|███▍      | 338/1000 [40:28<1:19:16,  7.18s/it, avr_loss=0.0971]steps:  34%|███▍      | 338/1000 [40:28<1:19:16,  7.18s/it, avr_loss=0.0974]steps:  34%|███▍      | 338/1000 [40:31<1:19:23,  7.20s/it, avr_loss=0.097] steps:  34%|███▍      | 339/1000 [40:35<1:19:09,  7.18s/it, avr_loss=0.097]steps:  34%|███▍      | 339/1000 [40:35<1:19:09,  7.18s/it, avr_loss=0.0982]steps:  34%|███▍      | 339/1000 [40:39<1:19:16,  7.20s/it, avr_loss=0.0957]steps:  34%|███▍      | 340/1000 [40:42<1:19:02,  7.19s/it, avr_loss=0.0957]steps:  34%|███▍      | 340/1000 [40:42<1:19:02,  7.19s/it, avr_loss=0.0955]steps:  34%|███▍      | 340/1000 [40:46<1:19:08,  7.20s/it, avr_loss=0.0964]steps:  34%|███▍      | 341/1000 [40:50<1:18:55,  7.19s/it, avr_loss=0.0964]steps:  34%|███▍      | 341/1000 [40:50<1:18:55,  7.19s/it, avr_loss=0.0968]steps:  34%|███▍      | 341/1000 [40:53<1:19:01,  7.20s/it, avr_loss=0.0961]steps:  34%|███▍      | 342/1000 [40:57<1:18:48,  7.19s/it, avr_loss=0.0961]steps:  34%|███▍      | 342/1000 [40:57<1:18:48,  7.19s/it, avr_loss=0.0958]steps:  34%|███▍      | 342/1000 [41:00<1:18:54,  7.20s/it, avr_loss=0.095] steps:  34%|███▍      | 343/1000 [41:04<1:18:41,  7.19s/it, avr_loss=0.095]steps:  34%|███▍      | 343/1000 [41:04<1:18:41,  7.19s/it, avr_loss=0.0951]steps:  34%|███▍      | 343/1000 [41:08<1:18:47,  7.20s/it, avr_loss=0.0953]steps:  34%|███▍      | 344/1000 [41:11<1:18:33,  7.19s/it, avr_loss=0.0953]steps:  34%|███▍      | 344/1000 [41:11<1:18:33,  7.19s/it, avr_loss=0.095] steps:  34%|███▍      | 344/1000 [41:15<1:18:40,  7.20s/it, avr_loss=0.0955]steps:  34%|███▍      | 345/1000 [41:19<1:18:26,  7.19s/it, avr_loss=0.0955]steps:  34%|███▍      | 345/1000 [41:19<1:18:26,  7.19s/it, avr_loss=0.0951]steps:  34%|███▍      | 345/1000 [41:22<1:18:33,  7.20s/it, avr_loss=0.0952]steps:  35%|███▍      | 346/1000 [41:26<1:18:19,  7.19s/it, avr_loss=0.0952]steps:  35%|███▍      | 346/1000 [41:26<1:18:19,  7.19s/it, avr_loss=0.0949]steps:  35%|███▍      | 346/1000 [41:30<1:18:26,  7.20s/it, avr_loss=0.0959]steps:  35%|███▍      | 347/1000 [41:33<1:18:12,  7.19s/it, avr_loss=0.0959]steps:  35%|███▍      | 347/1000 [41:33<1:18:12,  7.19s/it, avr_loss=0.0957]steps:  35%|███▍      | 347/1000 [41:37<1:18:19,  7.20s/it, avr_loss=0.0973]steps:  35%|███▍      | 348/1000 [41:41<1:18:05,  7.19s/it, avr_loss=0.0973]steps:  35%|███▍      | 348/1000 [41:41<1:18:05,  7.19s/it, avr_loss=0.098] steps:  35%|███▍      | 348/1000 [41:44<1:18:12,  7.20s/it, avr_loss=0.0975]steps:  35%|███▍      | 349/1000 [41:48<1:17:58,  7.19s/it, avr_loss=0.0975]steps:  35%|███▍      | 349/1000 [41:48<1:17:58,  7.19s/it, avr_loss=0.0978]steps:  35%|███▌      | 350/1000 [41:52<1:17:45,  7.18s/it, avr_loss=0.0978]steps:  35%|███▌      | 350/1000 [41:52<1:17:45,  7.18s/it, avr_loss=0.0964]
epoch 11/29

2025-10-02 15:38:28 INFO     epoch is incremented. current_epoch: 0, epoch: 11                                train_util.py:779
2025-10-02 15:38:28 INFO     epoch is incremented. current_epoch: 0, epoch: 11                                train_util.py:779
2025-10-02 15:38:28 INFO     epoch is incremented. current_epoch: 0, epoch: 11                                train_util.py:779
2025-10-02 15:38:28 INFO     epoch is incremented. current_epoch: 0, epoch: 11                                train_util.py:779
steps:  35%|███▌      | 350/1000 [41:56<1:17:52,  7.19s/it, avr_loss=0.0957]steps:  35%|███▌      | 351/1000 [41:59<1:17:39,  7.18s/it, avr_loss=0.0957]steps:  35%|███▌      | 351/1000 [41:59<1:17:39,  7.18s/it, avr_loss=0.0953]steps:  35%|███▌      | 351/1000 [42:03<1:17:45,  7.19s/it, avr_loss=0.0959]steps:  35%|███▌      | 352/1000 [42:07<1:17:32,  7.18s/it, avr_loss=0.0959]steps:  35%|███▌      | 352/1000 [42:07<1:17:32,  7.18s/it, avr_loss=0.0966]steps:  35%|███▌      | 352/1000 [42:10<1:17:38,  7.19s/it, avr_loss=0.0969]steps:  35%|███▌      | 353/1000 [42:14<1:17:24,  7.18s/it, avr_loss=0.0969]steps:  35%|███▌      | 353/1000 [42:14<1:17:24,  7.18s/it, avr_loss=0.0982]steps:  35%|███▌      | 353/1000 [42:17<1:17:31,  7.19s/it, avr_loss=0.0994]steps:  35%|███▌      | 354/1000 [42:21<1:17:17,  7.18s/it, avr_loss=0.0994]steps:  35%|███▌      | 354/1000 [42:21<1:17:17,  7.18s/it, avr_loss=0.1]   steps:  35%|███▌      | 354/1000 [42:25<1:17:24,  7.19s/it, avr_loss=0.0999]steps:  36%|███▌      | 355/1000 [42:28<1:17:10,  7.18s/it, avr_loss=0.0999]steps:  36%|███▌      | 355/1000 [42:28<1:17:10,  7.18s/it, avr_loss=0.101] steps:  36%|███▌      | 355/1000 [42:32<1:17:17,  7.19s/it, avr_loss=0.101]steps:  36%|███▌      | 356/1000 [42:36<1:17:03,  7.18s/it, avr_loss=0.101]steps:  36%|███▌      | 356/1000 [42:36<1:17:03,  7.18s/it, avr_loss=0.1]  steps:  36%|███▌      | 356/1000 [42:39<1:17:10,  7.19s/it, avr_loss=0.0998]steps:  36%|███▌      | 357/1000 [42:43<1:16:56,  7.18s/it, avr_loss=0.0998]steps:  36%|███▌      | 357/1000 [42:43<1:16:56,  7.18s/it, avr_loss=0.1]   steps:  36%|███▌      | 357/1000 [42:46<1:17:03,  7.19s/it, avr_loss=0.101]steps:  36%|███▌      | 358/1000 [42:50<1:16:49,  7.18s/it, avr_loss=0.101]steps:  36%|███▌      | 358/1000 [42:50<1:16:49,  7.18s/it, avr_loss=0.0998]steps:  36%|███▌      | 358/1000 [42:54<1:16:56,  7.19s/it, avr_loss=0.101] steps:  36%|███▌      | 359/1000 [42:57<1:16:42,  7.18s/it, avr_loss=0.101]steps:  36%|███▌      | 359/1000 [42:57<1:16:42,  7.18s/it, avr_loss=0.102]steps:  36%|███▌      | 359/1000 [43:01<1:16:49,  7.19s/it, avr_loss=0.103]steps:  36%|███▌      | 360/1000 [43:05<1:16:35,  7.18s/it, avr_loss=0.103]steps:  36%|███▌      | 360/1000 [43:05<1:16:35,  7.18s/it, avr_loss=0.102]steps:  36%|███▌      | 360/1000 [43:08<1:16:42,  7.19s/it, avr_loss=0.102]steps:  36%|███▌      | 361/1000 [43:12<1:16:28,  7.18s/it, avr_loss=0.102]steps:  36%|███▌      | 361/1000 [43:12<1:16:28,  7.18s/it, avr_loss=0.101]steps:  36%|███▌      | 361/1000 [43:15<1:16:35,  7.19s/it, avr_loss=0.1]  steps:  36%|███▌      | 362/1000 [43:19<1:16:21,  7.18s/it, avr_loss=0.1]steps:  36%|███▌      | 362/1000 [43:19<1:16:21,  7.18s/it, avr_loss=0.1]steps:  36%|███▌      | 362/1000 [43:23<1:16:28,  7.19s/it, avr_loss=0.1]steps:  36%|███▋      | 363/1000 [43:26<1:16:14,  7.18s/it, avr_loss=0.1]steps:  36%|███▋      | 363/1000 [43:26<1:16:14,  7.18s/it, avr_loss=0.0998]steps:  36%|███▋      | 363/1000 [43:30<1:16:20,  7.19s/it, avr_loss=0.101] steps:  36%|███▋      | 364/1000 [43:34<1:16:07,  7.18s/it, avr_loss=0.101]steps:  36%|███▋      | 364/1000 [43:34<1:16:07,  7.18s/it, avr_loss=0.101]steps:  36%|███▋      | 364/1000 [43:37<1:16:13,  7.19s/it, avr_loss=0.101]steps:  36%|███▋      | 365/1000 [43:41<1:16:00,  7.18s/it, avr_loss=0.101]steps:  36%|███▋      | 365/1000 [43:41<1:16:00,  7.18s/it, avr_loss=0.103]steps:  36%|███▋      | 365/1000 [43:45<1:16:06,  7.19s/it, avr_loss=0.103]steps:  37%|███▋      | 366/1000 [43:48<1:15:53,  7.18s/it, avr_loss=0.103]steps:  37%|███▋      | 366/1000 [43:48<1:15:53,  7.18s/it, avr_loss=0.102]steps:  37%|███▋      | 366/1000 [43:52<1:15:59,  7.19s/it, avr_loss=0.103]steps:  37%|███▋      | 367/1000 [43:56<1:15:46,  7.18s/it, avr_loss=0.103]steps:  37%|███▋      | 367/1000 [43:56<1:15:46,  7.18s/it, avr_loss=0.102]steps:  37%|███▋      | 367/1000 [43:59<1:15:52,  7.19s/it, avr_loss=0.102]steps:  37%|███▋      | 368/1000 [44:03<1:15:39,  7.18s/it, avr_loss=0.102]steps:  37%|███▋      | 368/1000 [44:03<1:15:39,  7.18s/it, avr_loss=0.103]steps:  37%|███▋      | 368/1000 [44:06<1:15:45,  7.19s/it, avr_loss=0.104]steps:  37%|███▋      | 369/1000 [44:10<1:15:32,  7.18s/it, avr_loss=0.104]steps:  37%|███▋      | 369/1000 [44:10<1:15:32,  7.18s/it, avr_loss=0.104]steps:  37%|███▋      | 369/1000 [44:14<1:15:38,  7.19s/it, avr_loss=0.106]steps:  37%|███▋      | 370/1000 [44:17<1:15:25,  7.18s/it, avr_loss=0.106]steps:  37%|███▋      | 370/1000 [44:17<1:15:25,  7.18s/it, avr_loss=0.106]steps:  37%|███▋      | 370/1000 [44:21<1:15:31,  7.19s/it, avr_loss=0.106]steps:  37%|███▋      | 371/1000 [44:25<1:15:18,  7.18s/it, avr_loss=0.106]steps:  37%|███▋      | 371/1000 [44:25<1:15:18,  7.18s/it, avr_loss=0.106]steps:  37%|███▋      | 371/1000 [44:28<1:15:24,  7.19s/it, avr_loss=0.106]steps:  37%|███▋      | 372/1000 [44:32<1:15:11,  7.18s/it, avr_loss=0.106]steps:  37%|███▋      | 372/1000 [44:32<1:15:11,  7.18s/it, avr_loss=0.105]steps:  37%|███▋      | 372/1000 [44:36<1:15:17,  7.19s/it, avr_loss=0.105]steps:  37%|███▋      | 373/1000 [44:39<1:15:04,  7.18s/it, avr_loss=0.105]steps:  37%|███▋      | 373/1000 [44:39<1:15:04,  7.18s/it, avr_loss=0.104]steps:  37%|███▋      | 373/1000 [44:43<1:15:10,  7.19s/it, avr_loss=0.105]steps:  37%|███▋      | 374/1000 [44:47<1:14:57,  7.18s/it, avr_loss=0.105]steps:  37%|███▋      | 374/1000 [44:47<1:14:57,  7.18s/it, avr_loss=0.105]steps:  37%|███▋      | 374/1000 [44:50<1:15:03,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 375/1000 [44:54<1:14:50,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 375/1000 [44:54<1:14:50,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 375/1000 [44:57<1:14:56,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 376/1000 [45:00<1:14:42,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 376/1000 [45:00<1:14:42,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 376/1000 [45:04<1:14:48,  7.19s/it, avr_loss=0.106]steps:  38%|███▊      | 377/1000 [45:08<1:14:35,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 377/1000 [45:08<1:14:35,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 377/1000 [45:11<1:14:41,  7.19s/it, avr_loss=0.106]steps:  38%|███▊      | 378/1000 [45:15<1:14:28,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 378/1000 [45:15<1:14:28,  7.18s/it, avr_loss=0.104]steps:  38%|███▊      | 378/1000 [45:18<1:14:33,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 379/1000 [45:22<1:14:21,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 379/1000 [45:22<1:14:21,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 379/1000 [45:26<1:14:26,  7.19s/it, avr_loss=0.106]steps:  38%|███▊      | 380/1000 [45:29<1:14:14,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 380/1000 [45:29<1:14:14,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 380/1000 [45:33<1:14:19,  7.19s/it, avr_loss=0.107]steps:  38%|███▊      | 381/1000 [45:37<1:14:07,  7.18s/it, avr_loss=0.107]steps:  38%|███▊      | 381/1000 [45:37<1:14:07,  7.18s/it, avr_loss=0.108]steps:  38%|███▊      | 381/1000 [45:40<1:14:12,  7.19s/it, avr_loss=0.107]steps:  38%|███▊      | 382/1000 [45:44<1:14:00,  7.18s/it, avr_loss=0.107]steps:  38%|███▊      | 382/1000 [45:44<1:14:00,  7.18s/it, avr_loss=0.107]steps:  38%|███▊      | 382/1000 [45:48<1:14:05,  7.19s/it, avr_loss=0.105]steps:  38%|███▊      | 383/1000 [45:51<1:13:52,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 383/1000 [45:51<1:13:52,  7.18s/it, avr_loss=0.105]steps:  38%|███▊      | 383/1000 [45:55<1:13:58,  7.19s/it, avr_loss=0.106]steps:  38%|███▊      | 384/1000 [45:59<1:13:45,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 384/1000 [45:59<1:13:45,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 385/1000 [46:02<1:13:33,  7.18s/it, avr_loss=0.106]steps:  38%|███▊      | 385/1000 [46:02<1:13:33,  7.18s/it, avr_loss=0.106]
epoch 12/29

2025-10-02 15:42:39 INFO     epoch is incremented. current_epoch: 0, epoch: 12                                train_util.py:779
2025-10-02 15:42:39 INFO     epoch is incremented. current_epoch: 0, epoch: 12                                train_util.py:779
2025-10-02 15:42:39 INFO     epoch is incremented. current_epoch: 0, epoch: 12                                train_util.py:779
2025-10-02 15:42:39 INFO     epoch is incremented. current_epoch: 0, epoch: 12                                train_util.py:779
steps:  38%|███▊      | 385/1000 [46:06<1:13:39,  7.19s/it, avr_loss=0.106]steps:  39%|███▊      | 386/1000 [46:10<1:13:26,  7.18s/it, avr_loss=0.106]steps:  39%|███▊      | 386/1000 [46:10<1:13:26,  7.18s/it, avr_loss=0.105]steps:  39%|███▊      | 386/1000 [46:14<1:13:32,  7.19s/it, avr_loss=0.107]steps:  39%|███▊      | 387/1000 [46:17<1:13:19,  7.18s/it, avr_loss=0.107]steps:  39%|███▊      | 387/1000 [46:17<1:13:19,  7.18s/it, avr_loss=0.107]steps:  39%|███▊      | 387/1000 [46:21<1:13:25,  7.19s/it, avr_loss=0.107]steps:  39%|███▉      | 388/1000 [46:25<1:13:12,  7.18s/it, avr_loss=0.107]steps:  39%|███▉      | 388/1000 [46:25<1:13:12,  7.18s/it, avr_loss=0.106]steps:  39%|███▉      | 388/1000 [46:28<1:13:18,  7.19s/it, avr_loss=0.105]steps:  39%|███▉      | 389/1000 [46:32<1:13:05,  7.18s/it, avr_loss=0.105]steps:  39%|███▉      | 389/1000 [46:32<1:13:05,  7.18s/it, avr_loss=0.105]steps:  39%|███▉      | 389/1000 [46:35<1:13:11,  7.19s/it, avr_loss=0.105]steps:  39%|███▉      | 390/1000 [46:39<1:12:58,  7.18s/it, avr_loss=0.105]steps:  39%|███▉      | 390/1000 [46:39<1:12:58,  7.18s/it, avr_loss=0.103]steps:  39%|███▉      | 390/1000 [46:43<1:13:04,  7.19s/it, avr_loss=0.102]steps:  39%|███▉      | 391/1000 [46:46<1:12:51,  7.18s/it, avr_loss=0.102]steps:  39%|███▉      | 391/1000 [46:46<1:12:51,  7.18s/it, avr_loss=0.102]steps:  39%|███▉      | 391/1000 [46:50<1:12:57,  7.19s/it, avr_loss=0.103]steps:  39%|███▉      | 392/1000 [46:54<1:12:44,  7.18s/it, avr_loss=0.103]steps:  39%|███▉      | 392/1000 [46:54<1:12:44,  7.18s/it, avr_loss=0.103]steps:  39%|███▉      | 392/1000 [46:57<1:12:50,  7.19s/it, avr_loss=0.104]steps:  39%|███▉      | 393/1000 [47:01<1:12:37,  7.18s/it, avr_loss=0.104]steps:  39%|███▉      | 393/1000 [47:01<1:12:37,  7.18s/it, avr_loss=0.104]steps:  39%|███▉      | 393/1000 [47:04<1:12:43,  7.19s/it, avr_loss=0.102]steps:  39%|███▉      | 394/1000 [47:08<1:12:30,  7.18s/it, avr_loss=0.102]steps:  39%|███▉      | 394/1000 [47:08<1:12:30,  7.18s/it, avr_loss=0.101]steps:  39%|███▉      | 394/1000 [47:12<1:12:36,  7.19s/it, avr_loss=0.102]steps:  40%|███▉      | 395/1000 [47:15<1:12:23,  7.18s/it, avr_loss=0.102]steps:  40%|███▉      | 395/1000 [47:15<1:12:23,  7.18s/it, avr_loss=0.103]steps:  40%|███▉      | 395/1000 [47:19<1:12:29,  7.19s/it, avr_loss=0.103]steps:  40%|███▉      | 396/1000 [47:23<1:12:16,  7.18s/it, avr_loss=0.103]steps:  40%|███▉      | 396/1000 [47:23<1:12:16,  7.18s/it, avr_loss=0.103]steps:  40%|███▉      | 396/1000 [47:26<1:12:22,  7.19s/it, avr_loss=0.104]steps:  40%|███▉      | 397/1000 [47:30<1:12:09,  7.18s/it, avr_loss=0.104]steps:  40%|███▉      | 397/1000 [47:30<1:12:09,  7.18s/it, avr_loss=0.104]steps:  40%|███▉      | 397/1000 [47:34<1:12:14,  7.19s/it, avr_loss=0.103]steps:  40%|███▉      | 398/1000 [47:37<1:12:02,  7.18s/it, avr_loss=0.103]steps:  40%|███▉      | 398/1000 [47:37<1:12:02,  7.18s/it, avr_loss=0.102]steps:  40%|███▉      | 398/1000 [47:41<1:12:07,  7.19s/it, avr_loss=0.101]steps:  40%|███▉      | 399/1000 [47:45<1:11:55,  7.18s/it, avr_loss=0.101]steps:  40%|███▉      | 399/1000 [47:45<1:11:55,  7.18s/it, avr_loss=0.101]steps:  40%|███▉      | 399/1000 [47:48<1:12:00,  7.19s/it, avr_loss=0.101]steps:  40%|████      | 400/1000 [47:52<1:11:48,  7.18s/it, avr_loss=0.101]steps:  40%|████      | 400/1000 [47:52<1:11:48,  7.18s/it, avr_loss=0.1]  steps:  40%|████      | 400/1000 [47:55<1:11:53,  7.19s/it, avr_loss=0.101]steps:  40%|████      | 401/1000 [47:59<1:11:41,  7.18s/it, avr_loss=0.101]steps:  40%|████      | 401/1000 [47:59<1:11:41,  7.18s/it, avr_loss=0.101]steps:  40%|████      | 401/1000 [48:03<1:11:46,  7.19s/it, avr_loss=0.101]steps:  40%|████      | 402/1000 [48:06<1:11:34,  7.18s/it, avr_loss=0.101]steps:  40%|████      | 402/1000 [48:06<1:11:34,  7.18s/it, avr_loss=0.103]steps:  40%|████      | 402/1000 [48:10<1:11:39,  7.19s/it, avr_loss=0.103]steps:  40%|████      | 403/1000 [48:14<1:11:27,  7.18s/it, avr_loss=0.103]steps:  40%|████      | 403/1000 [48:14<1:11:27,  7.18s/it, avr_loss=0.103]steps:  40%|████      | 403/1000 [48:17<1:11:32,  7.19s/it, avr_loss=0.103]steps:  40%|████      | 404/1000 [48:20<1:11:19,  7.18s/it, avr_loss=0.103]steps:  40%|████      | 404/1000 [48:20<1:11:19,  7.18s/it, avr_loss=0.104]steps:  40%|████      | 404/1000 [48:24<1:11:24,  7.19s/it, avr_loss=0.102]steps:  40%|████      | 405/1000 [48:27<1:11:12,  7.18s/it, avr_loss=0.102]steps:  40%|████      | 405/1000 [48:27<1:11:12,  7.18s/it, avr_loss=0.104]steps:  40%|████      | 405/1000 [48:31<1:11:17,  7.19s/it, avr_loss=0.104]steps:  41%|████      | 406/1000 [48:35<1:11:05,  7.18s/it, avr_loss=0.104]steps:  41%|████      | 406/1000 [48:35<1:11:05,  7.18s/it, avr_loss=0.104]steps:  41%|████      | 406/1000 [48:38<1:11:10,  7.19s/it, avr_loss=0.105]steps:  41%|████      | 407/1000 [48:42<1:10:58,  7.18s/it, avr_loss=0.105]steps:  41%|████      | 407/1000 [48:42<1:10:58,  7.18s/it, avr_loss=0.105]steps:  41%|████      | 407/1000 [48:46<1:11:03,  7.19s/it, avr_loss=0.106]steps:  41%|████      | 408/1000 [48:49<1:10:51,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 408/1000 [48:49<1:10:51,  7.18s/it, avr_loss=0.107]steps:  41%|████      | 408/1000 [48:53<1:10:56,  7.19s/it, avr_loss=0.106]steps:  41%|████      | 409/1000 [48:57<1:10:43,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 409/1000 [48:57<1:10:43,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 409/1000 [49:00<1:10:49,  7.19s/it, avr_loss=0.106]steps:  41%|████      | 410/1000 [49:04<1:10:36,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 410/1000 [49:04<1:10:36,  7.18s/it, avr_loss=0.105]steps:  41%|████      | 410/1000 [49:07<1:10:42,  7.19s/it, avr_loss=0.106]steps:  41%|████      | 411/1000 [49:11<1:10:29,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 411/1000 [49:11<1:10:29,  7.18s/it, avr_loss=0.106]steps:  41%|████      | 411/1000 [49:15<1:10:34,  7.19s/it, avr_loss=0.105]steps:  41%|████      | 412/1000 [49:18<1:10:22,  7.18s/it, avr_loss=0.105]steps:  41%|████      | 412/1000 [49:18<1:10:22,  7.18s/it, avr_loss=0.107]steps:  41%|████      | 412/1000 [49:22<1:10:27,  7.19s/it, avr_loss=0.106]steps:  41%|████▏     | 413/1000 [49:26<1:10:15,  7.18s/it, avr_loss=0.106]steps:  41%|████▏     | 413/1000 [49:26<1:10:15,  7.18s/it, avr_loss=0.106]steps:  41%|████▏     | 413/1000 [49:29<1:10:20,  7.19s/it, avr_loss=0.106]steps:  41%|████▏     | 414/1000 [49:33<1:10:08,  7.18s/it, avr_loss=0.106]steps:  41%|████▏     | 414/1000 [49:33<1:10:08,  7.18s/it, avr_loss=0.108]steps:  41%|████▏     | 414/1000 [49:37<1:10:13,  7.19s/it, avr_loss=0.108]steps:  42%|████▏     | 415/1000 [49:40<1:10:01,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 415/1000 [49:40<1:10:01,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 415/1000 [49:44<1:10:06,  7.19s/it, avr_loss=0.106]steps:  42%|████▏     | 416/1000 [49:48<1:09:54,  7.18s/it, avr_loss=0.106]steps:  42%|████▏     | 416/1000 [49:48<1:09:54,  7.18s/it, avr_loss=0.105]steps:  42%|████▏     | 416/1000 [49:51<1:09:59,  7.19s/it, avr_loss=0.106]steps:  42%|████▏     | 417/1000 [49:55<1:09:47,  7.18s/it, avr_loss=0.106]steps:  42%|████▏     | 417/1000 [49:55<1:09:47,  7.18s/it, avr_loss=0.107]steps:  42%|████▏     | 417/1000 [49:58<1:09:52,  7.19s/it, avr_loss=0.108]steps:  42%|████▏     | 418/1000 [50:02<1:09:40,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 418/1000 [50:02<1:09:40,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 418/1000 [50:06<1:09:45,  7.19s/it, avr_loss=0.107]steps:  42%|████▏     | 419/1000 [50:09<1:09:33,  7.18s/it, avr_loss=0.107]steps:  42%|████▏     | 419/1000 [50:09<1:09:33,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 420/1000 [50:13<1:09:21,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 420/1000 [50:13<1:09:21,  7.18s/it, avr_loss=0.107]
epoch 13/29

2025-10-02 15:46:50 INFO     epoch is incremented. current_epoch: 0, epoch: 13                                train_util.py:779
2025-10-02 15:46:50 INFO     epoch is incremented. current_epoch: 0, epoch: 13                                train_util.py:779
2025-10-02 15:46:50 INFO     epoch is incremented. current_epoch: 0, epoch: 13                                train_util.py:779
2025-10-02 15:46:50 INFO     epoch is incremented. current_epoch: 0, epoch: 13                                train_util.py:779
steps:  42%|████▏     | 420/1000 [50:17<1:09:27,  7.19s/it, avr_loss=0.108]steps:  42%|████▏     | 421/1000 [50:21<1:09:15,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 421/1000 [50:21<1:09:15,  7.18s/it, avr_loss=0.107]steps:  42%|████▏     | 421/1000 [50:25<1:09:20,  7.19s/it, avr_loss=0.107]steps:  42%|████▏     | 422/1000 [50:28<1:09:08,  7.18s/it, avr_loss=0.107]steps:  42%|████▏     | 422/1000 [50:28<1:09:08,  7.18s/it, avr_loss=0.106]steps:  42%|████▏     | 422/1000 [50:32<1:09:13,  7.19s/it, avr_loss=0.108]steps:  42%|████▏     | 423/1000 [50:36<1:09:01,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 423/1000 [50:36<1:09:01,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 423/1000 [50:39<1:09:06,  7.19s/it, avr_loss=0.109]steps:  42%|████▏     | 424/1000 [50:43<1:08:54,  7.18s/it, avr_loss=0.109]steps:  42%|████▏     | 424/1000 [50:43<1:08:54,  7.18s/it, avr_loss=0.108]steps:  42%|████▏     | 424/1000 [50:46<1:08:59,  7.19s/it, avr_loss=0.109]steps:  42%|████▎     | 425/1000 [50:50<1:08:47,  7.18s/it, avr_loss=0.109]steps:  42%|████▎     | 425/1000 [50:50<1:08:47,  7.18s/it, avr_loss=0.11] steps:  42%|████▎     | 425/1000 [50:54<1:08:52,  7.19s/it, avr_loss=0.11]steps:  43%|████▎     | 426/1000 [50:57<1:08:40,  7.18s/it, avr_loss=0.11]steps:  43%|████▎     | 426/1000 [50:57<1:08:40,  7.18s/it, avr_loss=0.109]steps:  43%|████▎     | 426/1000 [51:01<1:08:45,  7.19s/it, avr_loss=0.108]